<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>机器学习 | GXBLOGS</title><meta name="author" content="ggw &amp; xpl"><meta name="copyright" content="ggw &amp; xpl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="三天入门机器学习 1.1 人工智能概述 1.1.1 机器学习与人工智能 深度学习 达特茅斯会议-人工智能的起点 机器学习是人工智能的一个实现途径 深度学习是机器学习的一个方法发展而来 1.1.2机器学习、深度学习能做些什么 传统预测 量化投资 广告推荐 图像识别 自然语言处理 文本分类 情感分析 文本检测 1.2什么是机器学习 1.2.1 定义 从数据中自动分析获得模型 并利用模型对未知数据进行预">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习">
<meta property="og:url" content="https://gxblogs.com/posts/498ab7d9/index.html">
<meta property="og:site_name" content="GXBLOGS">
<meta property="og:description" content="三天入门机器学习 1.1 人工智能概述 1.1.1 机器学习与人工智能 深度学习 达特茅斯会议-人工智能的起点 机器学习是人工智能的一个实现途径 深度学习是机器学习的一个方法发展而来 1.1.2机器学习、深度学习能做些什么 传统预测 量化投资 广告推荐 图像识别 自然语言处理 文本分类 情感分析 文本检测 1.2什么是机器学习 1.2.1 定义 从数据中自动分析获得模型 并利用模型对未知数据进行预">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_112943.jpg">
<meta property="article:published_time" content="2023-08-20T06:55:23.079Z">
<meta property="article:modified_time" content="2024-01-20T09:17:13.816Z">
<meta property="article:author" content="ggw &amp; xpl">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_112943.jpg"><link rel="shortcut icon" href="/static/imgs/paipai.ico"><link rel="canonical" href="https://gxblogs.com/posts/498ab7d9/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/pluginsSrc/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/pluginsSrc/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: ggw & xpl","link":"链接: ","source":"来源: GXBLOGS","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: '/pluginsSrc/flickr-justified-gallery/dist/fjGallery.min.js',
      css: '/pluginsSrc/flickr-justified-gallery/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-20 17:17:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/ggwsettings.css"><link rel="stylesheet" href="/css/nav_menu.css"><link rel="stylesheet" href="/css/highlight/Kimbiedark.css"><script src="/css/else/echarts.min.js"></script><script src="/css/else/wow.min.js"></script><script type="text/javascript" src="/css/else/echarts-gl.min.js"></script><script src="/css/else/pace.min.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/static/imgs/loading.gif" data-original="/static/imgs/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">63</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-comments"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_112943.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">GXBLOGS</a></span><div id="menus"></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-comments"></i><span> 留言板</span></a></div></div><div id="nav-right"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-20T06:55:23.079Z" title="发表于 2023-08-20 14:55:23">2023-08-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-20T09:17:13.816Z" title="更新于 2024-01-20 17:17:13">2024-01-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>45分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/498ab7d9/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/498ab7d9/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>三天入门机器学习</h1>
<h2 id="1-1-人工智能概述">1.1 人工智能概述</h2>
<h3 id="1-1-1-机器学习与人工智能-深度学习">1.1.1 机器学习与人工智能 深度学习</h3>
<p>达特茅斯会议-人工智能的起点<br>
机器学习是人工智能的一个实现途径<br>
深度学习是机器学习的一个方法发展而来</p>
<h3 id="1-1-2机器学习、深度学习能做些什么">1.1.2机器学习、深度学习能做些什么</h3>
<p><strong>传统预测</strong> 量化投资 广告推荐<br>
<strong>图像识别</strong><br>
<strong>自然语言处理</strong> 文本分类 情感分析 文本检测</p>
<h2 id="1-2什么是机器学习">1.2什么是机器学习</h2>
<h3 id="1-2-1-定义">1.2.1 定义</h3>
<p>从<strong>数据</strong>中<strong>自动分析获得模型</strong> 并利用<strong>模型</strong>对未知数据进行<strong>预测</strong></p>
<h3 id="1-2-3-数据集构成">1.2.3 数据集构成</h3>
<p><strong>结构：特征值+目标值</strong></p>
<p>每一行数据称为样本</p>
<p>有些数据集可以没有目标值</p>
<h2 id="1-3-机器学习算法分类">1.3 机器学习算法分类</h2>
<h3 id="1-3-1-监督学习">1.3.1 监督学习</h3>
<p>​       <strong>目标值</strong>：类别 - 分类问题</p>
<p>​       <strong>目标值</strong>：连续型数据-回归问题</p>
<p><strong>目标值</strong>：无-无监督学习</p>
<ul>
<li>
<p><strong>监督学习</strong></p>
<p>分类k-近邻算法 贝叶斯分类 决策树随机森林 逻辑回归</p>
<p>回归 线性回归 岭回归</p>
</li>
<li>
<p><strong>无监督学习</strong></p>
<p>K-MEANS</p>
</li>
</ul>
<h2 id="1-4-机器学习开发流程">1.4 机器学习开发流程</h2>
<p><strong>获取数据 -&gt; 数据处理 -&gt; 特征工程-&gt; 机器学习算法训练-&gt;模型 -&gt;模型评估</strong></p>
<h2 id="1-5-学习框架和资料介绍">1.5 学习框架和资料介绍</h2>
<p><strong>算法</strong>是核心 数据与计算是基础</p>
<p><strong>找准定位</strong><br>
怎么做？<br>
1、<strong>入门</strong><br>
2、<strong>实战类书籍</strong><br>
3、<strong>机器学习 -”西瓜书”- 周志华</strong><br>
<strong>统计学习方法 - 李航</strong><br>
<strong>深度学习 - “花书”</strong></p>
<h3 id="1-5-1-机器学习库与框架">1.5.1 机器学习库与框架</h3>
<h2 id="2-1-数据集">2.1 数据集</h2>
<ul>
<li>目标
<ul>
<li>直到数据集分为训练集和测试集</li>
<li>会使用<strong>sklearn</strong>的数据集</li>
</ul>
</li>
</ul>
<h3 id="2-1-1可用数据集">2.1.1可用数据集</h3>
<p>公司内部 百度</p>
<p>数据接口 花钱</p>
<p>数据集</p>
<p>Kaggle:www.kaggle.com/datasets</p>
<p>UCI 数据集: <a target="_blank" rel="noopener" href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a></p>
<p>scikit-learn: <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/datasets/index.html#datasets">http://scikit-learn.org/stable/datasets/index.html#datasets</a></p>
<p>数据接口 花钱<br>
数据集<br>
学习阶段可以用的数据集：<br>
1）sklearn - 数据量小 方便学习<br>
2）kaggle 大数据竞赛平台<br>
3）UCI</p>
<h3 id="Scikit-learn工具介绍">Scikit-learn工具介绍</h3>
<p>classification -&gt;分类</p>
<p>regression -&gt;回归</p>
<p>clustering-&gt;聚类</p>
<p>dimensionality reduction -&gt;降维</p>
<p>model selection -&gt;模型选择</p>
<p>preprocessing -&gt;特征工程</p>
<h3 id="2-1-2-sklearn-数据集">2.1.2 sklearn 数据集</h3>
<h4 id="1-scikit-learn-数据集api介绍">1 scikit-learn 数据集api介绍</h4>
<ul>
<li>sklearn.datasets
<ul>
<li>加载获取流行的数据集</li>
<li>dataset.load_*()
<ul>
<li>获取小规模数据集 数据包含在datasets里</li>
</ul>
</li>
<li>datasets.fetch_*(data_home = None)
<ul>
<li>获取大规模数据集 从网络上下载 函数第一个参数是data_home表示数据集下载的目录 默认是~/scikit_learn_data/</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-sklearn-小数据集">2 sklearn 小数据集</h4>
<ul>
<li>
<p>sklearn.datasets.load_iris()</p>
<p>加载并返回鸢尾花数据集</p>
</li>
<li>
<p>sklearn.datasets.load_boston()</p>
<p>加载并返回波士顿房价数据集</p>
</li>
</ul>
<h4 id="3-sklearn大数据集">3 sklearn大数据集</h4>
<ul>
<li>sklearn.datasets.fetch_20newsgroups(data_home = None,subset = ‘train’)
<ul>
<li>subset: ‘train’或者’test’ 'all’可选 选择要加载的数据集</li>
<li>训练集的训练 测试集的测试</li>
</ul>
</li>
</ul>
<h4 id="4-sklearn数据集的使用">4 sklearn数据集的使用</h4>
<ul>
<li>
<p>load 和 fetch 返回的数据类型databases.base.Bunch（字典格式）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">dict[&quot;key&quot;] = values<br>bunch.key = values<br></code></pre></td></tr></table></figure>
<ul>
<li>
<p>data:特征数据数组 是[n_samples *n _features]的二维numpy.ndarray数组</p>
</li>
<li>
<p>target:标签数组 是n_samples的一维numpy.ndarray数组</p>
</li>
<li>
<p>DESCR:数据描述</p>
</li>
<li>
<p>feature_names:特征名 新闻数据 手写数字 回归数据集没有</p>
</li>
<li>
<p>target_name:标签名</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">datasets_demo</span>():<br> <span class="hljs-comment"># 获取数据集</span><br> iris = load_iris()<br> <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;鸢尾花数据集：\n&quot;</span>, iris)<br> <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;查看数据集描述：\n&quot;</span>, iris[<span class="hljs-string">&quot;DESCR&quot;</span>])<br> <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;查看特征值的名字：\n&quot;</span>,iris.feature_names)<br>   <span class="hljs-comment">#150个样本 4个特征</span><br> <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;查看特征值：\n&quot;</span>,iris.data,iris.data.shape)<br> <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>   <span class="hljs-comment"># 代码1：数据集使用</span><br>  datasets_demo()<br>   <br></code></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<p>拿到的数据是否可以全部用来训练一个模型？</p>
<h3 id="2-1-3-数据集的划分">2.1.3 数据集的划分</h3>
<ul>
<li>训练数据：用于训练，构建模型</li>
<li>测试数据：在模型检验时使用，用于评估模型是否有效</li>
</ul>
<p><strong>划分比例：</strong></p>
<ul>
<li>
<p>训练集 70% 80% 75%</p>
</li>
<li>
<p>测试集 30% 20% 30%</p>
</li>
<li>
<p>sklearn.model_selection.train_test_split(arrays, *options)</p>
</li>
<li>
<p>训练集特征值，测试集特征值，训练集目标值，测试集目标值<br>
x_train                 x_test,                      y_train            y_test</p>
</li>
<li>
<p>x 数据集的特征值</p>
</li>
<li>
<p>y数据集的标签值</p>
</li>
<li>
<p>test_size 测试集大小 一般为float</p>
</li>
<li>
<p>random_state 随机数种子 不同种子会造成不同的随机采样结果 相同的种子采样结果相同</p>
</li>
<li>
<p>return 训练集特征值 测试集特征值 训练集目标值 测试集目标值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">datasets_demo</span>():<br>    <span class="hljs-comment"># 获取数据集</span><br>    iris = load_iris()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;鸢尾花数据集：\n&quot;</span>, iris)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;查看数据集描述：\n&quot;</span>, iris[<span class="hljs-string">&quot;DESCR&quot;</span>])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;查看特征值的名字：\n&quot;</span>, iris.feature_names)<br>    <span class="hljs-comment"># 150个样本 4个特征</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;查看特征值：\n&quot;</span>, iris.data, iris.data.shape)<br>    <span class="hljs-comment"># 数据集划分</span><br>    x_train,x_test,y_train,y_test = train_test_split(iris.data, iris.target, test_size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">22</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练集的特征值：\n&quot;</span>, x_train, x_train.shape)<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    <span class="hljs-comment"># 代码1：数据集使用</span><br>    datasets_demo()<br><br></code></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="2-2-特征工程介绍">2.2 特征工程介绍</h2>
<p>算法 特征工程</p>
<h3 id="2-2-1-为什么需要">2.2.1 为什么需要</h3>
<p>数据与特征决定上限 模型和算法只是逼近上限</p>
<h3 id="2-2-2-什么是特征工程">2.2.2 什么是特征工程</h3>
<p><strong>sklearn 特征工程</strong><br>
pandas 数据清洗、数据处理<br>
特征抽取/特征提取<br>
<strong>机器学习算法</strong> - <strong>统计方法</strong> - <strong>数学公式</strong><br>
文本类型 -&gt; 数值<br>
类型 -&gt; 数值</p>
<h3 id="2-3-1-特征提取">2.3.1 特征提取</h3>
<h4 id="1-将任意数据（文本、图像）转换为可用于机器学习的数字特征">1 将任意数据（文本、图像）转换为可用于机器学习的数字特征</h4>
<blockquote>
<p>注：特征值化是为了计算机更好的去理解</p>
</blockquote>
<ul>
<li>
<p>字典特征提取（特征离散化）</p>
</li>
<li>
<p>文本特征提取</p>
</li>
<li>
<p>图像特征提取</p>
</li>
</ul>
<h4 id="2-特征提取API">2 特征提取API</h4>
<p><code>sklearn.feature_extraction</code></p>
<h3 id="2-3-2-字典特征提取-类别-one-hot编码">2.3.2 字典特征提取 -类别-&gt;one-hot编码</h3>
<p>作用：对字典数据特征值化</p>
<ul>
<li>
<p><code>sklearn.feature_extraction.DictVectorizer(sparse=True,…)</code></p>
</li>
<li>
<p><strong>vector</strong> 数学：向量 物理：矢量<br>
矩阵 matrix 二维数组<br>
<strong>向量 vector 一维数组</strong></p>
</li>
<li>
<p>父类：转换器类</p>
</li>
<li>
<pre><code>DictVectorizer.fit_transform(X):字典或者字典的迭代器返回值：返回sparse矩阵
DictVectorizer.inverse_transform(X):array数组或者sparse矩阵：返回值：转换之前的数据格式
DictVectorizer.get_feature_names_out():返回类别名称
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>![snipaste20230717_112943](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_112943.jpg)<br><br>**sparse稀疏**<br>**将非零值-&gt;按位置表示出来**<br>**节省内存-&gt;提高加载效率**<br><br>![snipaste20230717_114105](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_114105.jpg)<br><br>```py<br>from sklearn.feature_extraction import DictVectorizer<br>def dict_demo():<br>    &quot;&quot;&quot;<br>    字典特征抽取<br>    &quot;&quot;&quot;<br>    data = [&#123;&#x27;city&#x27;: &#x27;北京&#x27;,&#x27;temperature&#x27;:100&#125;,&#123;&#x27;city&#x27;: &#x27;上海&#x27;,&#x27;temperature&#x27;:60&#125;,&#123;&#x27;city&#x27;: &#x27;深圳&#x27;,&#x27;temperature&#x27;:30&#125;]<br>    # 1、 实例化一个转换器类<br>    transfer = DictVectorizer(sparse=False)<br>    # 2、调用fit_transform()<br>    data_new = transfer.fit_transform(data)<br>    print(&quot;data_new:\n&quot;,data_new)<br>    print(&quot;特征名字：\n&quot;,transfer.get_feature_names_out())<br>    return None<br>if __name__ == &quot;__main__&quot;:<br>     dict_demo()<br></code></pre></td></tr></table></figure>

</code></pre>
</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_114611.jpg" alt="snipaste20230717_114611"><br>
<strong>应用场景：</strong><br>
1.<strong>pclass, sex 数据集当中类别特征比较多</strong><br>
1、将数据集的特征-&gt;字典类型<br>
2、DictVectorizer 转换</p>
<pre><code>2) **本身拿到的数据就是字典类型**
</code></pre>
<h3 id="2-3-3-文本特征提取">2.3.3 文本特征提取</h3>
<p>单词作为特征</p>
<p>句子 短语 单词<br>
特征：特征词</p>
<h4 id="方法1：CountVectorizer">方法1：CountVectorizer</h4>
<ul>
<li><code>sklearn.feature_extraction.text.CountVectorizer(stop words = [])</code></li>
<li>stop_words 停用的 对最终分类没有用处</li>
<li>停用词表</li>
</ul>
<p>​    返回词频矩阵</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">CountVectorizer.fit_transform(X):文本或者文本的迭代器返回值：返回sparse矩阵<br>CountVectorizer.inverse_transform(X):array数组或者sparse矩阵：返回值：转换之前的数据格式<br>CountVectorizer.get_feature_names_out():返回值：单词列表<br></code></pre></td></tr></table></figure>
<p>案例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_demo</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        文本特征抽取</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><span class="hljs-comment">#     实例化转换器类</span><br>    transfer = CountVectorizer()<br><span class="hljs-comment">#    调用fit_transform</span><br>    data = [<span class="hljs-string">&quot;life is short,i like like python&quot;</span>, <span class="hljs-string">&quot;life is too long,i dislike python&quot;</span>]<br>    data_new = transfer.fit_transform(data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;data_new:\n&#x27;</span>,data_new.toarray())<br>    <span class="hljs-comment"># 如果要看到二维数组的结果 在后面调用toarray()这个函数就可以啦</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>
<ul>
<li>
<p>统计每个样本特征词出现的个数</p>
</li>
<li>
<p>中文转换器 需要使用空格</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def count_chinese_demo():<br>    &quot;&quot;&quot;<br>    中文文本特征抽取：CountVecotrizer<br>    :return:<br>    &quot;&quot;&quot;<br>    data = [&quot;我 爱 北京 天安门&quot;, &quot;天安门 上 太阳 升&quot;]<br>    # 1、实例化一个转换器类<br>    transfer = CountVectorizer()<br><br>    # 2、调用fit_transform<br>    data_new = transfer.fit_transform(data)<br>    print(&quot;data_new:\n&quot;, data_new.toarray())<br>    print(&quot;特征名字：\n&quot;, transfer.get_feature_names())<br><br>    return None<br><br><br></code></pre></td></tr></table></figure>
<p>注意：如果要调用特征名字 得用实例化对象调用 如果是查询具体数据 用调用后得出的结果</p>
</li>
</ul>
<h4 id="方法2：-jieba这一自动分节器">方法2： jieba这一自动分节器</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#记得导包 此处省略了</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_chinese_demo2</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    中文文本特征抽取，自动分词</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 将中文文本进行分词</span><br>    data = [<span class="hljs-string">&quot;一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。&quot;</span>,<br>            <span class="hljs-string">&quot;我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。&quot;</span>,<br>            <span class="hljs-string">&quot;如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。&quot;</span>]<br><br>    data_new = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> data:<br>        data_new.append(cut_word(sent))<br>    <span class="hljs-comment"># print(data_new)</span><br>    <span class="hljs-comment"># 1、实例化一个转换器类</span><br>    transfer = CountVectorizer(stop_words=[<span class="hljs-string">&quot;一种&quot;</span>, <span class="hljs-string">&quot;所以&quot;</span>])<br><br>    <span class="hljs-comment"># 2、调用fit_transform</span><br>    data_final = transfer.fit_transform(data_new)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;data_new:\n&quot;</span>, data_final.toarray())<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;特征名字：\n&quot;</span>, transfer.get_feature_names())<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    <span class="hljs-comment">#中文分词</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">cut_word</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    进行中文分词：&quot;我爱北京天安门&quot; --&gt; &quot;我 爱 北京 天安门&quot;</span><br><span class="hljs-string">    :param text:</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot; &quot;</span>.join(<span class="hljs-built_in">list</span>(jieba.cut(text)))<br></code></pre></td></tr></table></figure>
<p>ps: 装包的时候 一般是要进入那个路径的 以电脑3.11这个版本为例</p>
<p>cd venv\Scripts 找到那个activate.bat 启动它 才能开始安装</p>
<ul>
<li>关键词：在某一个类别的文章中出现很多 但其他类别的文章中出现很少</li>
</ul>
<h4 id="方法3：-TfidfVectorizer">方法3： TfidfVectorizer</h4>
<ul>
<li>
<p><strong>TF-IDF</strong>:评估某一字词重要程度</p>
</li>
<li>
<p><strong>词频（TF）</strong>: 某一个给定的词语在一篇文章中出现的频率</p>
</li>
<li>
<p><strong>逆向文档频率(IDF)</strong>：总文件数目除以包含该词语文件的数目 再取以10为底的对数</p>
</li>
<li>
<p><strong>TF-IDF</strong> = <strong>TF*IDF</strong></p>
</li>
<li>
<p>两个词 “经济”，“非常”<br>
1000篇文章-语料库<br>
100篇文章 - “非常”<br>
10篇文章 - “经济”<br>
两篇文章<br>
文章A(100词) : 10次“经济” TF-IDF:0.2相乘<br>
tf:10/100 = 0.1<br>
idf:lg 1000/10 = 2<br>
文章B(100词) : 10次“非常” TF-IDF:0.1<br>
tf:10/100 = 0.1<br>
idf: log 10 1000/100 = 1<br>
对数？<br>
2 ^ 3 = 8<br>
log 2 8 = 3<br>
log 10 10 = 1</p>
</li>
</ul>
<h4 id="API">API</h4>
<ul>
<li>
<p><code>sklearn.feature_extraction.text.TfidVectorizer(stop_words=None,…)</code></p>
</li>
<li>
<ul>
<li>返回词的权重矩阵
<ul>
<li>TfidfVectorizer.fit_transform(X):文本或者文本的迭代器返回值：返回sparse矩阵</li>
<li>TfidfVectorizer.inverse_transform(X):array数组或者sparse矩阵：返回值：转换之前的数据格- T封TfidfVectorizer.get_feature_names_out():返回值：单词列表</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>案例和自动分节器差不多不再重复了</p>
<h2 id="2-4-特征预处理">2.4 特征预处理</h2>
<h3 id="2-4-1-定义">2.4.1 定义</h3>
<p><strong>通过一些转换函数将特征数据转换成更适合算法模型的特征数据过程</strong></p>
<h4 id="1-包含内容">1 包含内容</h4>
<ul>
<li>
<p>数值型数据量纲化：</p>
<ul>
<li>
<p>归一化</p>
</li>
<li>
<p>标准化</p>
<p>不同规格的数据转换成统一规格 单位大小差距过大</p>
</li>
</ul>
</li>
</ul>
<h4 id="2-特征预处理API">2 特征预处理API</h4>
<p><code>sklearn.preprocessing</code></p>
<h3 id="2-4-2-归一化">2.4.2 归一化</h3>
<h4 id="1-定义">1 定义</h4>
<p>通过对原始数据进行变换把数据映射到0-1区间</p>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_200509.jpg" alt="snipaste20230717_200509"></p>
<h4 id="2-API">2 API</h4>
<ul>
<li>
<p><code>sklearn.preprocessing.MinMaxScaler(feature_range = (0,1)...)</code></p>
<ul>
<li>
<p>MinMaxscalar.fit_transform(X)</p>
<ul>
<li>
<p>X: numpy array 格式的数据[n_samples,n_features]</p>
<p>返回值：转换后的形状相同的array</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="3-数据计算">3 数据计算</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import pandas as pd<br>from sklearn.preprocessing import MinMaxScaler<br>def minmax_demo():<br>    &quot;&quot;&quot;<br>    归一化<br>    :return:<br>    &quot;&quot;&quot;<br>    #1、获取数据<br>    data=pd.read_csv(&quot;dating.txt&quot;)<br>    data = data.iloc[:,:3]<br>    # 选择前三列的数据 索引方式<br>    print(&quot;data:\n&quot;, data)<br>    #2.实例化一个转换器类<br>    transfer = MinMaxScaler()<br>    #3.调用fit_transform<br>    data_new = transfer.fit_transform(data)<br>    print(&quot;data_new:\n&quot;,data_new)<br></code></pre></td></tr></table></figure>
<p>劣处：出现异常值 比如极大极小 那么最终结果也会不准</p>
<p>本身就是依靠最大最小求值 鲁棒性差 精确性差</p>
<h3 id="2-4-3-标准化">2.4.3 标准化</h3>
<h4 id="1-定义-2">1 定义</h4>
<p>原始数据进行变换把数据变换到均值为0 标准差为1</p>
<h4 id="2-公式">2 公式</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">(x - mean) / std<br>std:标准差 集中程度<br>mead:平均值<br></code></pre></td></tr></table></figure>
<p>出现异常点也不需要担忧</p>
<p>方差改变较小</p>
<h4 id="3-API">3 API</h4>
<p><code>sklearn.preprocessing.StandardScaler(feature_range = (0,1)...)</code></p>
<ul>
<li>
<p>StandardScalar.fit_transform(X)</p>
<ul>
<li>
<p>X: numpy array 格式的数据[n_samples,n_features]</p>
<p>返回值：转换后的形状相同的array</p>
</li>
</ul>
</li>
</ul>
<h4 id="4-数据计算">4 数据计算</h4>
<p>类似 不再赘述 适合大数据场景</p>
<h2 id="2-5-特征降维">2.5 特征降维</h2>
<h3 id="2-5-1-降维">2.5.1 降维</h3>
<p>ndarray</p>
<p>​      维数：嵌套的层数</p>
<p>​      0维：标量</p>
<p>​      1维：向量</p>
<p>​      2维：矩阵</p>
<p>二维数组</p>
<p>在某些限定条件下 降低随机变量（特征）个数 得到一组主变量</p>
<p>效果：要求特征与特征之间不相关</p>
<h3 id="2-5-2-降维的两种方式">2.5.2 降维的两种方式</h3>
<ul>
<li>特征选择</li>
<li>主成分分析</li>
</ul>
<h3 id="2-5-3-什么是特征选择">2.5.3 什么是特征选择</h3>
<h4 id="1-定义-3">1 定义</h4>
<p>数据中包含冗余或者相关变量（特征） 在原有特征中找到主要特征</p>
<h4 id="2-方法">2 方法</h4>
<ul>
<li>
<p>Filter(过滤式)：主要探究特征本身特点 特征与特征和目标值之间关联</p>
<ul>
<li>方差过滤法：低方差特征过滤（所有鸟都有爪子）</li>
<li>相关系数 - 特征与特征之间的相关程度</li>
</ul>
</li>
<li>
<p>Embedded(嵌入式)：算法自动选择特征（特征与目标</p>
<ul>
<li>
<p>决策树：信息熵 信息增益</p>
</li>
<li>
<p>正则化：L1 L2</p>
</li>
<li>
<p>深度学习：卷积等</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>嵌入式只能讲解算法时候介绍</p>
</blockquote>
<h4 id="3-模块">3 模块</h4>
<p><code>sklearn.feature_selection</code></p>
<h4 id="4-过滤式">4 过滤式</h4>
<h5 id="4-1-低方差特征过滤">4.1 低方差特征过滤</h5>
<h6 id="4-1-1-API">4.1.1 API</h6>
<p><code>sklearn.feature_selection.VarianceThreshold(threshold = 0.0)</code></p>
<ul>
<li>
<p>VarianceThreshold.fit_transform(X)</p>
<ul>
<li>
<p>X: numpy array 格式的数据[n_samples,n_features]</p>
<p>返回值：训练集差异低于threshold的特征将被删除 默认值保留所有非零方差特征</p>
</li>
</ul>
</li>
</ul>
<h6 id="4-1-2-实例">4.1.2 实例</h6>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def variance_demo():<br>    data = pd.read_csv(&quot;factor_returns.csv&quot;)<br><br>    data = data.iloc[:,1:-2]<br>    print(&quot;data:\n&quot;, data)<br>    transfer = VarianceThreshold()<br>    data_new = transfer.fit_transform(data)<br>    print(&quot;data_new:\n&quot;,data_new)<br></code></pre></td></tr></table></figure>
<h5 id="4-2-相关系数">4.2 相关系数</h5>
<p>皮尔森相关系数 pearson</p>
<p>公式：</p>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_210603.jpg" alt="snipaste20230717_210603"></p>
<h6 id="4-2-3-特点">4.2.3 特点</h6>
<p>相关系数介于-1到+1之间</p>
<ul>
<li>r&gt;0 两变量正相关 r &lt; 0 两变量负相关</li>
<li>|r| = 1两变量完全相关 r = 0 两变量无相关关系</li>
<li>|r|越接近1，线性关系越密切 接近0 线性相关越弱</li>
<li>&lt;0.4 低度相关 0.4-0.7 显著性相关 &gt;0.7 高度相关</li>
</ul>
<h6 id="4-2-4-API">4.2.4 API</h6>
<ul>
<li>from scipy.stats import pearson
<ul>
<li>x：</li>
<li>y:</li>
</ul>
</li>
</ul>
<p>两个特征的相关性</p>
<h6 id="4-2-5-股票案例">4.2.5 股票案例</h6>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">r = pearsonr(data[&quot;pe_ratio&quot;],data[&quot;pb_ratio&quot;])<br>    print(&quot;相关系数：\n&quot;,r)<br></code></pre></td></tr></table></figure>
<p>可以利用for循环输出多对</p>
<p>特征与特征相关性很高</p>
<ul>
<li>
<p>选取其中一个</p>
</li>
<li>
<p>按一定权重加权求和</p>
</li>
<li>
<p>主成分分析</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import matpilotlib.pyplot as plt<br>plt.figure(figsize = (20,8),dpi = 100)<br>plt.scatter(data[&#x27;revenue&#x27;],data[&#x27;total_expense&#x27;])<br>plt.show()<br></code></pre></td></tr></table></figure>
<h2 id="2-6-主成分分析">2.6 主成分分析</h2>
<h3 id="2-6-1-什么是主成分分析（PCA）">2.6.1 什么是主成分分析（PCA）</h3>
<p>高维数据转化为低维数据 舍弃原有数据 创造新的变量</p>
<p>降低原数据复杂度</p>
<p>回归分析或者聚类分析</p>
<h4 id="1-计算案例">1 计算案例</h4>
<img src="/static/imgs/loading.gif" data-original="D:\_xpl2022\Desktop\markdown\picture\snipaste20230717_212949.jpg" alt="snipaste20230717_212949" style="zoom:50%;" />
<p>五个点变成三个点 二维变一维</p>
<h4 id="2-API-2">2 API</h4>
<ul>
<li>
<p><code>sklearn.decomposition.PCA(n_components = None)</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">n_components<br>    小数 表示保留百分之多少的信息<br>    整数 减少到多少特征<br></code></pre></td></tr></table></figure>
<ul>
<li>
<p>PCA.fit_transform(X)</p>
</li>
<li>
<p>返回值：转换后制定维度的array</p>
</li>
</ul>
</li>
</ul>
<h4 id="3-数据计算-2">3 数据计算</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from sklearn.decomposition import  PCA<br>def pca_demo():<br>    data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]<br>    transfer = PCA(n_components=0.95)<br>    <br>    data_new = transfer.fit_transform(data)<br>    print(&quot;data_new=\n&quot;,data_new)<br>    return None<br></code></pre></td></tr></table></figure>
<h3 id="2-6-2-探究用户对物品类别的喜好细分">2.6.2 探究用户对物品类别的喜好细分</h3>
<h4 id="1-需求">1 需求</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">用户          物品类别<br>user_id       aisle<br>1）需要将user_id和aisle放在同一个表中 - 合并<br>2）找到user_id和aisle - 交叉表和透视表<br>3）特征冗余过多 -&gt; PCA降维<br></code></pre></td></tr></table></figure>
<h4 id="2-分析">2 分析</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#1.获取数据</span><br><span class="hljs-comment">#2.合并表</span><br><span class="hljs-comment">#3.找到user_id 和 aisle之间的关系</span><br><span class="hljs-comment">#4.PCA降维</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br>order_products = pd.read_csv(<span class="hljs-string">&quot;./instacart/order_products__prior.csv&quot;</span>)<br>products = pd.read_csv(<span class="hljs-string">&quot;./instacart/products.csv&quot;</span>)<br>orders = pd.read_csv(<span class="hljs-string">&quot;./instacart/orders.csv&quot;</span>)<br>aisles = pd.read_csv(<span class="hljs-string">&quot;./instacart/aisles.csv&quot;</span>)<br><span class="hljs-comment">#合并aisles  products</span><br>tab1 = pd.merge(aisles,products,on=[<span class="hljs-string">&quot;aisle_id&quot;</span>,<span class="hljs-string">&quot;aisle_id&quot;</span>])<br>tab2 = pd.merge(tab1,order_products,on=[<span class="hljs-string">&quot;product_id&quot;</span>,<span class="hljs-string">&quot;product_id&quot;</span>])<br>tab3 = pd.merge(tab2,orders,on=[<span class="hljs-string">&quot;order_id&quot;</span>,<span class="hljs-string">&quot;order_id&quot;</span>])<br>tab3.head()<br><span class="hljs-comment"># 交叉表</span><br>table = pd.crosstab(tab3[<span class="hljs-string">&quot;user_id&quot;</span>],tab3[<span class="hljs-string">&quot;aisle&quot;</span>])<br>data = table[:<span class="hljs-number">10000</span>]<br>transfer = PCA(n_components=<span class="hljs-number">0.95</span>)<br>data_new = transfer.fit_transform(data)<br><span class="hljs-comment"># print(data_new)</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;特征类别：\n&quot;</span>,data_new.shape)<br><br></code></pre></td></tr></table></figure>
<h2 id="3-1-sklearn转换器和估计器">3.1 sklearn转换器和估计器</h2>
<h3 id="3-1-1-转换器">3.1.1 转换器</h3>
<p><strong>转换器调用形式</strong></p>
<ul>
<li>
<p>fit_transform</p>
</li>
<li>
<p>fit</p>
</li>
<li>
<p>transform</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1 实例化 (实例化的是一个转换器类(Transformer))<br>2 调用fit_transform(对于文档建立分类词频矩阵，不能同时调用)<br>标准化：<br>    (x - mean) / std<br>    fit_transform()<br>        fit()           计算 每一列的平均值、标准差<br>        transform()     (x - mean) / std进行最终的转换<br></code></pre></td></tr></table></figure>
<h3 id="3-1-2-估计器estimator">3.1.2 估计器estimator</h3>
<p>在sklearn中 实现算法api</p>
<img src="/static/imgs/loading.gif" data-original="D:\_xpl2022\Desktop\markdown\picture\snipaste20230718_155044.jpg" alt="snipaste20230718_155044" style="zoom: 67%;" />
<p>流程图：</p>
<img src="/static/imgs/loading.gif" data-original="D:\_xpl2022\Desktop\markdown\picture\snipaste20230718_155130.jpg" alt="snipaste20230718_155130" style="zoom:50%;" />
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1 实例化一个estimator<br>2 estimator.fit(x_train, y_train) 计算<br>    —— 调用完毕，模型生成<br>3 模型评估：<br>    1）直接比对真实值和预测值<br>        y_predict = estimator.predict(x_test)<br>        #划分好测试集x_test<br>        y_test == y_predict<br>    2）计算准确率<br>        accuracy = estimator.score(x_test, y_test)<br></code></pre></td></tr></table></figure>
<h2 id="3-2-K-近邻算法-KNN">3.2 K-近邻算法 KNN</h2>
<h3 id="3-2-1-什么是KNN">3.2.1 什么是KNN</h3>
<h4 id="1-原理">1 原理</h4>
<p>你的邻居来判断你的类别</p>
<p>如果一个样本在特征空间的k个最相似（特征空间中最邻近）的样本中大多数属于某一个类别</p>
<p>k = 1 容易受到异常点的影响</p>
<ul>
<li>
<p>距离公式</p>
<p>两个样本的距离可以通过如下公式计算 欧式距离：</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_160104.jpg" alt="snipaste20230718_160104"><br>
曼哈顿距离 绝对值距离<br>
明可夫斯基距离</p>
</li>
</ul>
<h4 id="2-电影类型分析">2 电影类型分析</h4>
<p>假设有几部电影 六个样本 打斗镜头 接吻镜头 目标：类型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">k = 1 爱情片<br>k = 2 爱情片<br>……<br>k = 6 无法确定<br>k = 7 动作片<br>k值过大 容易受到样本不均衡的影响<br></code></pre></td></tr></table></figure>
<h4 id="3-问题">3 问题</h4>
<ul>
<li>如果取的电影数量不一样？会是什么结果？</li>
<li>结合前面约会数据 需要对数据怎么样预处理？</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">-无量纲化的处理<br>-标准化<br></code></pre></td></tr></table></figure>
<h3 id="3-2-2-KNN-API">3.2.2 KNN API</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=&#x27;auto&#x27;)<br>n_neighbors：k值 k_neighbors查询默认使用的邻居数<br>algorithm:&#123;‘auto’，‘ball_tree’，‘kd_tree’，‘brute’&#125; 搜索邻居啥的<br>auto 将根据传递给fit方法选择最合适的算法<br></code></pre></td></tr></table></figure>
<h3 id="3-2-3-案例">3.2.3 案例</h3>
<h4 id="1-数据集介绍">1 数据集介绍</h4>
<p>iris数据集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1）获取数据<br>2）数据集划分<br>3）特征工程<br>    标准化<br>4）KNN预估器流程<br>5）模型评估<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from sklearn.datasets import load_iris<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.neighbors import KNeighborsClassifier<br>def knn_iris():<br>    &quot;&quot;&quot;<br>    用knn算法对鸢尾花数据集进行分类<br>    :return:<br>    &quot;&quot;&quot;<br>    # 1）获取数据<br>    iris = load_iris()<br>    # 2）划分数据集<br>    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=6)<br>    # 3) 特征工程：标准化<br>    transfer = StandardScaler()<br>    x_train = transfer.fit_transform(x_train)<br>    # 因为如果使用测试集自己的平均值会出现偏差 所以仅仅使用transform<br>    x_test = transfer.transform(x_test)<br>    # 4) KNN算法预估器<br>    estimator = KNeighborsClassifier()<br>    estimator.fit(x_train,y_train)<br>    # 5) 模型评估<br>    # 方法一：直接比对实际值和预测值<br>    y_predict = estimator.predict(x_test)<br>    print(&quot;y_predict:\n&quot;,y_predict)<br>    print(&quot;直接比对真实值和预测值：\n&quot;,y_test == y_predict)<br>    # 方法二 ：计算准确率<br>    score = estimator.score(x_test,y_test)<br>    print(&quot;准确率为：\n&quot;,score)<br>    return None<br>if __name__ == &quot;__main__&quot;:<br>     knn_iris()<br></code></pre></td></tr></table></figure>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_162742.jpg" alt="snipaste20230718_162742"></p>
<p>不能使用太大的数据量 开销很大</p>
<h2 id="3-3-模型选择与调优">3.3 模型选择与调优</h2>
<h3 id="3-3-1-什么是交叉验证">3.3.1 什么是交叉验证</h3>
<img src="/static/imgs/loading.gif" data-original="D:\_xpl2022\Desktop\markdown\picture\snipaste20230718_163156.jpg" alt="snipaste20230718_163156"  />
<h4 id="1-分析">1 分析</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_163308.jpg" alt="snipaste20230718_163308"></p>
<h4 id="2-目的">2 目的</h4>
<p>模型准确可信</p>
<h3 id="3-3-2-超参数搜索-网格搜索（Grid-Search）">3.3.2 超参数搜索-网格搜索（Grid-Search）</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">k的取值<br>    [1, 3, 5, 7, 9, 11]<br>    暴力破解<br></code></pre></td></tr></table></figure>
<p>模型中有一些参数需要我们手动指定 于是就可以调优</p>
<h4 id="模型选择与调优API">模型选择与调优API</h4>
<img src="/static/imgs/loading.gif" data-original="D:\_xpl2022\Desktop\markdown\picture\snipaste20230718_163716.jpg" alt="snipaste20230718_163716" style="zoom: 67%;" />
<h3 id="3-3-3-鸢尾花改进">3.3.3 鸢尾花改进</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def knn_iris_gscv():<br>    &quot;&quot;&quot;<br>    用knn算法对鸢尾花数据集进行分类 添加网格搜索和交叉验证<br>    :return:<br>    &quot;&quot;&quot;<br>    # 1）获取数据<br>    iris = load_iris()<br>    # 2）划分数据集<br>    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=120)<br>    # 3) 特征工程：标准化<br>    transfer = StandardScaler()<br>    x_train = transfer.fit_transform(x_train)<br>    # 因为如果使用测试集自己的平均值会出现偏差 所以仅仅使用transform<br>    x_test = transfer.transform(x_test)<br>    # 4) KNN算法预估器<br>    estimator = KNeighborsClassifier()<br>    # 加入网格搜索和交叉验证<br>    param_dict = &#123;&quot;n_neighbors&quot;: [1,3,5,7,9,11]&#125;<br>    estimator = GridSearchCV(estimator,param_grid=param_dict,cv=10)<br>    estimator.fit(x_train,y_train)<br>    # 5) 模型评估<br>    # 方法一：直接比对实际值和预测值<br>    y_predict = estimator.predict(x_test)<br>    print(&quot;y_predict:\n&quot;,y_predict)<br>    print(&quot;直接比对真实值和预测值：\n&quot;,y_test == y_predict)<br>    # 方法二 ：计算准确率<br>    score = estimator.score(x_test,y_test)<br>    print(&quot;准确率为：\n&quot;,score)<br>    # 最佳参数： best_params_<br>    print(&quot;最佳参数：\n&quot;,estimator.best_params_)<br>    # 最佳结果： best_score_<br>    print(&quot;最佳结果：\n&quot;, estimator.best_score_)<br>    # 最佳估计器： best_estimator_<br>    print(&quot;最佳估计器：\n&quot;, estimator.best_estimator_)<br>    # 交叉验证结果： cv_results_<br>    print(&quot;交叉验证结果：\n&quot;, estimator.cv_results_)<br><br>    return None<br><br></code></pre></td></tr></table></figure>
<h3 id="3-3-4-预测facebook签到位置">3.3.4 预测facebook签到位置</h3>
<h4 id="1-数据集介绍-2">1 数据集介绍</h4>
<img src="/static/imgs/loading.gif" data-original="D:\_xpl2022\Desktop\markdown\picture\snipaste20230718_165924.jpg" alt="snipaste20230718_165924" style="zoom:67%;" />
<ul>
<li>row_id: 索引 （无意义）</li>
<li>x y:（人所在的位置）</li>
<li>accuracy:（定位的准确度）</li>
<li>time:（在哪个时间在定位位置）</li>
<li>place_id；用户将要签到的位置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">流程分析：<br>    1）获取数据<br>    2）数据处理<br>    目的：<br>        特征值 x<br>        目标值 y<br>        a.缩小数据范围<br>          2 &lt; x &lt; 2.5<br>          1.0 &lt; y &lt; 1.5<br>        b.time -&gt; 年月日时分秒<br>        c.过滤签到次数少的地点<br>        数据集划分<br>     3）特征工程：标准化<br>     4）KNN算法预估流程<br>     5）模型选择与调优<br>     6）模型评估<br></code></pre></td></tr></table></figure>
<h4 id="2-实际操作">2 实际操作</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_175626.jpg" alt="snipaste20230718_175626"></p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_175646.jpg" alt="snipaste20230718_175646"></p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_175808.jpg" alt="snipaste20230718_175735"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># 3) 特征工程：标准化<br>transfer = StandardScaler()<br>x_train = transfer.fit_transform(x_train)<br># 因为如果使用测试集自己的平均值会出现偏差 所以仅仅使用transform<br>x_test = transfer.transform(x_test)<br># 4) KNN算法预估器<br>estimator = KNeighborsClassifier()<br># 加入网格搜索和交叉验证<br>param_dict = &#123;&quot;n_neighbors&quot;: [3,5,7,9]&#125;<br>estimator = GridSearchCV(estimator,param_grid=param_dict,cv=3)<br>estimator.fit(x_train,y_train)<br># 5) 模型评估<br># 方法一：直接比对实际值和预测值<br>y_predict = estimator.predict(x_test)<br>print(&quot;y_predict:\n&quot;,y_predict)<br>print(&quot;直接比对真实值和预测值：\n&quot;,y_test == y_predict)<br># 方法二 ：计算准确率<br>score = estimator.score(x_test,y_test)<br>print(&quot;准确率为：\n&quot;,score)<br># 最佳参数： best_params_<br>print(&quot;最佳参数：\n&quot;,estimator.best_params_)<br># 最佳结果： best_score_<br>print(&quot;最佳结果：\n&quot;, estimator.best_score_)<br># 最佳估计器： best_estimator_<br>print(&quot;最佳估计器：\n&quot;, estimator.best_estimator_)<br># 交叉验证结果： cv_results_<br>print(&quot;交叉验证结果：\n&quot;, estimator.cv_results_)<br></code></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">y_predict:<br> [1732563460 4642646293 7038685199 ... 9386144778 1380291167 3974542317]<br>直接比对真实值和预测值：<br> 10123260    False<br>16370269     True<br>8987212     False<br>25350732     True<br>2350188     False<br>27448389    False<br>18266407     True<br>20832425    False<br>20061648    False<br>9937274      True<br>22854526     True<br>9059450     False<br>24798827    False<br>5357941     False<br>21559497    False<br>17593468    False<br>3565205     False<br>1525691     False<br>24745125     True<br>17098577     True<br>24135559     True<br>24747751    False<br>3562966     False<br>8754665     False<br>4945947     False<br>19454562     True<br>124153      False<br>16592306    False<br>19493243    False<br>14946077     True<br>            ...  <br>2576728      True<br>17692961     True<br>1354061     False<br>8557538     False<br>26498264    False<br>15611307     True<br>19636821     True<br>7009739     False<br>14926961     True<br>8277291     False<br>7855506      True<br>20458273     True<br>26659503    False<br>12580447    False<br>6001349     False<br>8852544      True<br>12590771    False<br>6977831     False<br>22776654    False<br>25406749    False<br>3223869     False<br>12451110    False<br>18903770    False<br>27155003    False<br>7637819      True<br>480605      False<br>11920745    False<br>2818989     False<br>11329355    False<br>17936911    False<br>Name: place_id, Length: 20228, dtype: bool<br>准确率为：<br> 0.3661755981807396<br>最佳参数：<br> &#123;&#x27;n_neighbors&#x27;: 7&#125;<br>最佳结果：<br> 0.33317952605385454<br>最佳估计器：<br> KNeighborsClassifier(algorithm=&#x27;auto&#x27;, leaf_size=30, metric=&#x27;minkowski&#x27;,<br>           metric_params=None, n_jobs=1, n_neighbors=7, p=2,<br>           weights=&#x27;uniform&#x27;)<br><br></code></pre></td></tr></table></figure>
<h2 id="3-4-朴素贝叶斯算法">3.4 朴素贝叶斯算法</h2>
<h3 id="3-4-1-什么是朴素贝叶斯算法">3.4.1 什么是朴素贝叶斯算法</h3>
<h3 id="3-4-2-概率基础">3.4.2 概率基础</h3>
<h3 id="3-4-3-联合概率-条件概率-相互独立">3.4.3 联合概率 条件概率 相互独立</h3>
<h3 id="3-4-4-贝叶斯公式">3.4.4 贝叶斯公式</h3>
<h4 id="1-公式">1 公式</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_180516.jpg" alt="snipaste20230718_180516"></p>
<ul>
<li>注：w为给定文档的特征值（频数估计 预测文档提供）c为文档类别</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"> 联合概率：包含多个条件，且所有条件同时成立的概率<br>    P(程序员, 匀称) P(程序员, 超重|喜欢)<br>    P(A, B)<br>    条件概率：就是事件A在另外一个事件B已经发生条件下的发生概率<br>    P(程序员|喜欢) P(程序员, 超重|喜欢)<br>    P(A|B)<br>    相互独立:<br>        P(A, B) = P(A)P(B) &lt;=&gt; 事件A与事件B相互独立<br>朴素？<br>    假设：特征与特征之间是相互独立<br>朴素贝叶斯算法：<br>    朴素 + 贝叶斯<br>应用场景：<br>    文本分类<br>    单词作为特征<br>拉普拉斯平滑系数<br></code></pre></td></tr></table></figure>
<h4 id="2-文章分类计算">2 文章分类计算</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230718_181218.jpg" alt="snipaste20230718_181218"></p>
<p>P© = 3/4 属于china类的概率</p>
<h4 id="3-拉普拉斯平滑系数">3 拉普拉斯平滑系数</h4>
<p>防止计算出的分类概率为 0</p>
<p>P(F1|C) =(Ni+a)/N+am</p>
<p>a 为指定系数一般为1 m为训练文档中统计出的特征词个数</p>
<h3 id="3-4-5-API">3.4.5 API</h3>
<ul>
<li><code>sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</code>
<ul>
<li>朴素贝叶斯分类</li>
<li>alpha:拉普拉斯平滑系数</li>
</ul>
</li>
</ul>
<h3 id="3-4-6-案例：20个新闻组">3.4.6 案例：20个新闻组</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1）获取数据<br>2）划分数据集<br>3）特征工程<br>    文本特征抽取<br>    TFIDF 单词抽取<br>4）朴素贝叶斯预估器流程<br>5）模型评估<br></code></pre></td></tr></table></figure>
<p>代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def news_demo():<br>    &quot;&quot;&quot;<br>    用朴素贝叶斯对新闻分类<br>    :return:<br>    &quot;&quot;&quot;<br>    #1 获取数据<br>    news = fetch_20newsgroups(subset=&quot;all&quot;)<br>    #2 划分数据集<br>    x_train,x_test,y_train,y_test = train_test_split(news.data,news.target)<br>    #3 特征工程<br>    transfer = TfidfVectorizer()<br>    x_train = transfer.fit_transform(x_train)<br>    x_test = transfer.transform(x_test)<br>    #4 朴素贝叶斯算法<br>    estimator = MultinomialNB()<br>    estimator.fit(x_train,y_train)<br>    #5<br>    # 方法一：直接比对实际值和预测值<br>    y_predict = estimator.predict(x_test)<br>    print(&quot;y_predict:\n&quot;, y_predict)<br>    print(&quot;直接比对真实值和预测值：\n&quot;, y_test == y_predict)<br>    # 方法二 ：计算准确率<br>    score = estimator.score(x_test, y_test)<br>    print(&quot;准确率为：\n&quot;, score)<br>    return None<br></code></pre></td></tr></table></figure>
<h3 id="3-4-7-总结">3.4.7 总结</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">优点：<br>    对缺失数据不太敏感，算法也比较简单，常用于文本分类。<br>    分类准确度高，速度快<br>缺点：<br>    由于使用了样本属性独立性的假设，所以如果特征属性有关联时其效果不好<br></code></pre></td></tr></table></figure>
<h2 id="3-5-决策树">3.5 决策树</h2>
<h3 id="3-5-1-认识决策树">3.5.1 认识决策树</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">如何高效的进行决策？<br>特征的先后顺序<br></code></pre></td></tr></table></figure>
<p>if-else</p>
<h3 id="3-5-2-决策树原理详解">3.5.2 决策树原理详解</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">已知 四个特征值 预测 是否贷款给某个人<br>先看房子，再工作 -&gt; 是否贷款 只看了两个特征<br>年龄，信贷情况，工作 看了三个特征<br></code></pre></td></tr></table></figure>
<p><strong>问题：如何对这些客户进行分类预测？</strong></p>
<h4 id="1-原理-2">1 原理</h4>
<ul>
<li>信息熵 信息增益</li>
</ul>
<h4 id="2-信息熵的定义-信息的衡量-信息量-信息熵">2 信息熵的定义 信息的衡量 - 信息量 - 信息熵</h4>
<ul>
<li>
<p>H的专业术语称为信息熵 单位为比特</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo></mrow><annotation encoding="application/x-tex">H(X) = -</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">−</span></span></span></span>P(xi)lobb(P(xi))的和</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">香农：消除随机不定性的东西<br>小明 年龄 “我今年18岁” - 信息<br>小华 ”小明明年19岁” - 不是信息<br>  bit<br>g(D,A) = H(D) - 条件熵H(D|A)<br></code></pre></td></tr></table></figure>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230719_202053.jpg" alt="snipaste20230719_202053"></p>
<p>结果是0.971</p>
<h4 id="3-决策树的划分依据之一——信息增益">3 决策树的划分依据之一——信息增益</h4>
<ul>
<li>定义与公式</li>
</ul>
<p>特征A对训练集D的信息增益g(D,A) 定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵的H(D|A)之差：</p>
<p>g(D,A) = H(D) - 条件熵H(D|A)</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230719_202524.jpg" alt="snipaste20230719_202524"></p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230719_202632.jpg" alt="snipaste20230719_202632"></p>
<h4 id="4-其他依据">4 其他依据</h4>
<ul>
<li>
<p>ID3</p>
<ul>
<li>信息增益最大的准则</li>
</ul>
</li>
<li>
<p>C4.5</p>
<ul>
<li>信息增益比 最大的准则</li>
</ul>
</li>
<li>
<p>CART</p>
<ul>
<li>
<p>分类树：基尼系数 最小的准则 在sklearn中可以划分的默认原则</p>
</li>
<li>
<p>优势：划分更加细致</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-5-3-决策树API">3.5.3 决策树API</h3>
<ul>
<li>
<p><code>class sklearn.tree.DecisionTreeClassifier(criterion = 'gini',</code></p>
</li>
<li>
<p><code>max_depth = None,random_state = None)</code></p>
<ul>
<li>
<p>决策树分类器</p>
</li>
<li>
<p>criterion:默认是‘gini’系数 可以选择信息增益的熵’entropy’</p>
</li>
<li>
<p>max_depth:树的深度大小</p>
</li>
<li>
<p>random_state:随机数种子</p>
</li>
</ul>
</li>
</ul>
<p>鸢尾花的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def decision_iris():<br>    #1)获取数据集<br>    iris = load_iris()<br>    #2）划分数据集<br>    x_train,x_test,y_train,y_test  = train_test_split(iris.data,iris.target,random_state=22)<br>    #3）决策树<br>    estimator = DecisionTreeClassifier(criterion=&#x27;entropy&#x27;)<br>    estimator.fit(x_train,y_train)<br>    #4）<br>    y_predict = estimator.predict(x_test)<br>    print(&quot;y_predict:\n&quot;, y_predict)<br>    print(&quot;直接比对真实值和预测值：\n&quot;, y_test == y_predict)<br>    # 方法二 ：计算准确率<br>    score = estimator.score(x_test, y_test)<br>    print(&quot;准确率为：\n&quot;, score)<br>    return None<br></code></pre></td></tr></table></figure>
<p>没有免费的午餐：总是想找到一个算法一劳永逸</p>
<p>样本数目太少了</p>
<h3 id="3-5-4-案例：泰坦尼克号乘客生存预测">3.5.4 案例：泰坦尼克号乘客生存预测</h3>
<ul>
<li>泰坦尼克号数据</li>
</ul>
<h4 id="1-乘坐班是指乘客班（1-2-3）-社会经济阶层的代表">1 乘坐班是指乘客班（1,2,3） 社会经济阶层的代表</h4>
<h4 id="2-其中age数据存在缺失">2 其中age数据存在缺失</h4>
<blockquote>
<p>数据：<a target="_blank" rel="noopener" href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</a></p>
</blockquote>
<h3 id="分析步骤">分析步骤</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">流程分析：<br>    特征值 目标值<br>    1）获取数据<br>    2）数据处理<br>        缺失值处理<br>        特征值 -&gt; 字典类型<br>    3）准备好特征值 目标值<br>    4）划分数据集<br>    5）特征工程：字典特征抽取<br>    6）决策树预估器流程<br>    7）模型评估<br></code></pre></td></tr></table></figure>
<p>选择若干个重要的特征</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from sklearn.model_selection import train_test_split<br>import pandas as pd<br>from sklearn.feature_extraction import DictVectorizer<br>from sklearn.tree import DecisionTreeClassifier,export_graphviz<br>def decision_tree():<br>    #获取数据<br>    data = pd.read_csv(&quot;http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt&quot;)<br>    #筛选特征值和目标值<br>    x = data[[&#x27;pclass&#x27;, &#x27;age&#x27;, &#x27;sex&#x27;, &#x27;name&#x27;, &#x27;room&#x27;]]<br>    y = data[&#x27;survived&#x27;]<br>    #2.数据处理 1）处理缺失值 填补先索引 填补平均值 <br>    x[&#x27;age&#x27;].fillna(x[&#x27;age&#x27;].mean(), inplace=True)#fillna(替换的值，inplace表示替换默认为False）<br>    #划分训练集和测试集<br>    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)<br>    #特征工程（类别信息进行ONE-HOT编码）准换成字典to_dict<br>    Dict = DictVectorizer(sparse=False)#对字典数据进行特征值化,不按照sparse矩阵展示<br>    x_train = Dict.fit_transform(x_train.to_dict(orient=&quot;records&quot;))#to_dict将数据转换成字典，orient=&quot;record&quot;默认按照行进行<br>    x_test = Dict.fit_transform(x_test.to_dict(orient=&quot;records&quot;))<br>    #决策树预测<br>    dt = DecisionTreeClassifier(max_depth=5)<br>    dt.fit(x_train, y_train)<br>    #准确率评价<br>    print(&quot;准确率为&quot;, dt.score(x_test, y_test))<br>    #导出决策树结构<br>    export_graphviz(dt, out_file=&#x27;./tree.dot&#x27;, feature_names=[&#x27;pclass&#x27;, &#x27;age&#x27;, &#x27;sex&#x27;, &#x27;name&#x27;, &#x27;room&#x27;])<br>    return None<br>if __name__== &quot;__main__&quot;:<br>    decision_tree()<br><br></code></pre></td></tr></table></figure>
<h3 id="3-5-3-决策树可视化">3.5.3 决策树可视化</h3>
<h4 id="1-保存树的结构到dot文件">1 保存树的结构到dot文件</h4>
<ul>
<li>sklearn.tree.export_graphviz()该函数可以导出DOT格式
<ul>
<li>tree.export_graphviz(estimator, out_file = “tree.dot”,feature_names = [‘’,'])</li>
</ul>
</li>
</ul>
<p><code>export_graphviz(dc, out_file = &quot;./tree.dot&quot;,feature_names = ['age','plcass=1'])</code></p>
<h4 id="2-网站结构">2 网站结构</h4>
<ul>
<li><a target="_blank" rel="noopener" href="http://webgraphviz.com/">http://webgraphviz.com/</a></li>
</ul>
<h3 id="3-5-6-总结">3.5.6 总结</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">优点：<br>    可视化 - 可解释能力强<br>缺点：<br>    容易产生过拟合<br></code></pre></td></tr></table></figure>
<h2 id="3-6-集成学习方法之随机森林">3.6 集成学习方法之随机森林</h2>
<h3 id="3-6-1-什么是集成学习方法">3.6.1 什么是集成学习方法</h3>
<p>集成学习通过建立几个模型组合的来解决单一预测问题 它的工作原理是生成多个分类器/模型 各自独立学习和作出预测</p>
<p>因此优于任何一个单分类作出预测</p>
<h3 id="3-6-2-什么是随机森林">3.6.2 什么是随机森林</h3>
<p>在机器学习中 <strong>随机森林是一个包含多个决策树的分类器</strong> 并且输出类别是由个别树输出的类别的众数而定</p>
<p>假如训练了5个树 有四个树结果为true 1个树结果为false 最终投票结果为true</p>
<h3 id="3-6-3-随机森林原理过程">3.6.3 随机森林原理过程</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">训练集：<br>N个样本<br>特征值 目标值<br>M个特征<br>随机<br>    两个随机<br>        训练集随机 - N个样本中随机有放回的抽样N个<br></code></pre></td></tr></table></figure>
<p>学习算法根据下列算法建造每棵树：</p>
<ul>
<li>
<p>用N来表示训练用例（样本）的个数 M表示特征数目</p>
<ul>
<li>
<p>1 一次随机选择一个样本 重复N次</p>
</li>
<li>
<p>2 随机选出m个特征 建立决策树</p>
</li>
</ul>
</li>
<li>
<p>采用bootstrap抽样</p>
<ul>
<li>
<pre><code> bootstrap 随机有放回抽样
        [1, 2, 3, 4, 5]
        新的树的训练集
        [2, 2, 3, 1, 5]
    特征随机 - 从M个特征中随机抽取m个特征
        M &gt;&gt; m
        降维
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="3-6-4-API">3.6.4 API</h3>
<ul>
<li>
<p>class sklearn.ensemble.RandomForestClassifier(n_estimator = 10,critetion = ‘gini’,</p>
</li>
<li>
<p>max_depth = None;bootstrap = True,random_state = None,min_samples_split = 2)</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230719_215108.jpg" alt="snipaste20230719_215108"></p>
</li>
</ul>
<h3 id="3-6-5-随机森林预测案例">3.6.5 随机森林预测案例</h3>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230719_223031.jpg" alt=""></p>
<p>5,8,15</p>
<p>超参数调优：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">gc = GridSearchCV(rf,param_grid = param,cv = 2)<br>gc.fit(x_train,y_train)<br></code></pre></td></tr></table></figure>
<h3 id="3-6-6-总结">3.6.6 总结</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">能够有效地运行在大数据集上，<br>处理具有高维特征的输入样本，而且不需要降维  <br></code></pre></td></tr></table></figure>
<h2 id="4-回归与聚类算法">4 回归与聚类算法</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">回归和聚类<br>线性回归<br>欠拟合与过拟合<br>岭回归<br>分类算法：逻辑回归<br>模型保存与加载<br>无监督学习 K-means算法<br></code></pre></td></tr></table></figure>
<h2 id="4-1线性回归">4.1线性回归</h2>
<h3 id="4-1-1-线性回归的内容">4.1.1 线性回归的内容</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">回归问题：<br>    目标值 - 连续型的数据<br></code></pre></td></tr></table></figure>
<ul>
<li>房价预测</li>
<li>销售额度预测</li>
<li>金融：贷款额度预测、利用线性回归</li>
</ul>
<h3 id="2-什么是线性回归">2 什么是线性回归</h3>
<ul>
<li>
<p>利用回归方程对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式</p>
</li>
<li>
<p>单变量回归 多变量回归</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">    线性关系<br>y = w1x1 + w2x2 + w3x3 + …… + wnxn + b（偏置）<br>  = wTx（逆） + b<br></code></pre></td></tr></table></figure>
</li>
</ul>
<p>w x 理解成矩阵</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">数据挖掘基础<br>y = kx + b<br>y = w1x1 + w2x2 + b<br>y = 0.7x1 + 0.3x2<br>期末成绩：0.7×考试成绩+0.3×平时成绩<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">广义线性模型<br>    非线性关系？<br>    线性模型<br>        自变量一次<br>         y = w1x1 + w2x2 + w3x3 + …… + wnxn + b<br>        参数一次<br>         y = w1x1 + w2x1^2 + w3x1^3 + w4x2^3 + …… + b<br>    线性关系&amp;线性模型<br>    线性关系一定是线性模型<br>    线性模型不一定是线性关系<br></code></pre></td></tr></table></figure>
<h3 id="4-1-2-线性回归的损失和优化原理（理解记忆）">4.1.2 线性回归的损失和优化原理（理解记忆）</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">目标：求模型参数<br>模型参数能够使得预测准确<br>真实关系：真实房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率<br>随意假定：预测房子价格 = 0.25×中心区域的距离 + 0.14×城市一氧化氮浓度 + 0.42×自住房平均房价 + 0.34×城镇犯罪率<br></code></pre></td></tr></table></figure>
<p>不断更新权重与损失 两者差距越小越接近</p>
<p><strong>如何衡量差距？</strong><br>
<strong>损失函数/cost/成本函数/目标函数</strong></p>
<ul>
<li>最小二乘法</li>
</ul>
<h4 id="1-损失函数">1 损失函数</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_170618.jpg" alt="snipaste20230720_170618"></p>
<h4 id="2-优化算法">2 优化算法</h4>
<p>如何去求模型中的W 使得损失最小？</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_170750.jpg" alt="snipaste20230720_170750"></p>
<p>直接求解w</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">拓展：<br>1)<br>    y = ax^2 + bx + c<br>    y&#x27; = 2ax + b = 0<br>    x = - b / 2a<br>2)<br>    a * b = 1<br>        b = 1 / a = a ^ -1<br>    A * B = E<br>    单位矩阵<br>    [[1, 0, 0],<br>    [0, 1, 0],<br>    [0, 0, 1]]<br>    B = A ^ -1<br></code></pre></td></tr></table></figure>
<ul>
<li>梯度下降</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_171233.jpg" alt="snipaste20230720_171233"></p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_171322.jpg" alt="snipaste20230720_171322"></p>
<h3 id="4-1-3-线性回归API">4.1.3 线性回归API</h3>
<ul>
<li>
<p><code>sklearn.linear_model.LinearRegression(fit_intercept = True)</code></p>
</li>
<li>
<p><code>sklearn.linear_model.SGDRegressor(loss = &quot;squared_loss&quot;,fit_intercept = True,learing_rate = 'invscaling',eta0 = 0.01)</code></p>
</li>
<li>
<p>不加上偏置只能过原点</p>
</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_171819.jpg" alt="snipaste20230720_171819"></p>
<ul>
<li>loss:损失函数</li>
<li>学习率：步长</li>
</ul>
<h3 id="4-1-4-波士顿房价预测">4.1.4 波士顿房价预测</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">流程：<br>    1）获取数据集<br>    2）划分数据集<br>    3）特征工程：<br>        无量纲化 - 标准化<br>    4）预估器流程<br>        fit() --&gt; 模型<br>        coef_ intercept_<br>    5）模型评估<br></code></pre></td></tr></table></figure>
<p>代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from sklearn.datasets import load_boston<br>from sklearn.model_selection import train_test_split<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.linear_model import LinearRegression,SGDRegressor<br>def linear1():<br>    &quot;&quot;&quot;<br><br>    :return:<br>    &quot;&quot;&quot;<br>    #1获取数据集<br>    boston = load_boston()<br>    #2 划分数据集<br>    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)<br>    #3 标准化<br>    transfer = StandardScaler()<br>    x_train = transfer.fit_transform(x_train)<br>    x_test = transfer.transform(x_test)<br>    #4 模型<br>    estimator = LinearRegression()<br>    estimator1 = SGDRegressor()<br>    estimator.fit(x_train,y_train)<br>    #5 评估模型<br>    print(&quot;权重系数为：\n&quot;,estimator.coef_)<br>    print(&quot;偏置为：\n&quot;,estimator.intercept_)<br>if __name__ == &quot;__main__&quot;:<br>    print()<br>    #自从1.2 版本后波士顿数据集给移出去了<br></code></pre></td></tr></table></figure>
<h4 id="1-回归性能评估">1 回归性能评估</h4>
<p>均方误差评价机制：</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_173555.jpg" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">from sklearn.preprocessing import StandardScaler<br>from sklearn.linear_model import LinearRegression,SGDRegressor<br>from sklearn.metrics import mean_squared_error<br></code></pre></td></tr></table></figure>
<p><strong>模型评估：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">y_predict = estimator.predict(x_test)<br>print(&#x27;预测房价为：\n&#x27;,y_predict)<br>error = mean_squared_error(y_test,y_predict)<br>print(&quot;正规方程的均方误差为:\n&quot;,error)<br></code></pre></td></tr></table></figure>
<p>值越小越好 表示误差程度</p>
<h4 id="2-对比">2 对比</h4>
<table>
<thead>
<tr>
<th style="text-align:center">梯度下降</th>
<th style="text-align:center">正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">需要选择学习率</td>
<td style="text-align:center">NO</td>
</tr>
<tr>
<td style="text-align:center">需要迭代求解</td>
<td style="text-align:center">一次求解</td>
</tr>
<tr>
<td style="text-align:center">特征数目较大可以使用</td>
<td style="text-align:center">计算方程时间复杂度高</td>
</tr>
</tbody>
</table>
<ul>
<li>选择：
<ul>
<li>小规模数据：
<ul>
<li>LinearRegression(不能解决拟合问题)</li>
<li>岭回归</li>
</ul>
</li>
<li>大规模数据：SGDRegressor</li>
</ul>
</li>
</ul>
<p>GD SGD SAG</p>
<h2 id="4-2-欠拟合和过拟合">4.2 欠拟合和过拟合</h2>
<p>训练集上没事 但测试集表现很差</p>
<p>那估计就是过拟合</p>
<h3 id="4-2-1-什么是过拟合与欠拟合">4.2.1 什么是过拟合与欠拟合</h3>
<ul>
<li>欠拟合 学习到的特征太少了</li>
<li>过拟合 学习到的特征太多了</li>
</ul>
<h3 id="定义">定义</h3>
<ul>
<li>过拟合：一个假设在训练集上能够获得比其他假设更好的拟合 但在测试集上不能很好拟合数据 所以出现过拟合 模型过于复杂</li>
<li>欠拟合：一个假设在训练集和测试集都不能很好的拟合数据 模型过简单</li>
</ul>
<h3 id="4-2-2-原因以及解决办法">4.2.2 原因以及解决办法</h3>
<ul>
<li>
<p>欠拟合原因以及解决办法</p>
<ul>
<li>
<p>原因：学习到数据的特征过少</p>
</li>
<li>
<p>解决办法：增加数据的特征数量</p>
</li>
</ul>
</li>
<li>
<p>过拟合原因以及解决方法</p>
<ul>
<li>原因：原始特征过多 存在一些嘈杂特征 模型过于复杂是因为模型尝试去兼顾每个测试数据点</li>
<li>解决办法：正则化</li>
</ul>
</li>
<li>
<p>减少高次项特征的影响</p>
</li>
</ul>
<h4 id="1-正则化">1 正则化</h4>
<ul>
<li>L2正则化（常用）
<ul>
<li>让一些W更小 接近0 那么就可以削弱了</li>
</ul>
</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_184844.jpg" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">L1<br>损失函数 + λ惩罚项<br>LASSO<br>L2 更常用<br>损失函数 + λ惩罚项<br>Ridge - 岭回归<br></code></pre></td></tr></table></figure>
<h2 id="4-3-岭回归">4.3 岭回归</h2>
<h3 id="4-3-1-带有L2正则化的线性回归——岭回归">4.3.1 带有L2正则化的线性回归——岭回归</h3>
<p>算法建立回归方程的时候 加上正则化 解决过拟合的效果</p>
<h4 id="1-API">1 API</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_203645.jpg" alt="snipaste20230720_203645"></p>
<p>Ridge方法相当于SGDRegressor(penalty = ‘l2’,loss = ‘squared_loss’),只不过这个实现了一个普通的梯度下降学习</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_204000.jpg" alt="snipaste20230720_204000"></p>
<h4 id="2-观察正则化程度的变化-对结果的影响">2 观察正则化程度的变化 对结果的影响</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_204058.jpg" alt=""></p>
<h4 id="3-波士顿房价预测">3 波士顿房价预测</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def linear3():<br>    &quot;&quot;&quot;<br>    岭回归对波士顿房价预测<br>    :return:<br>    &quot;&quot;&quot;<br>    #1获取数据集<br>    boston = load_boston()<br>    print(&quot;特征数量:\n&quot;,boston.data.shape)<br>    #2 划分数据集<br>    x_train,x_test,y_train,y_test = train_test_split(boston.data,boston.target,random_state=22)<br>    #3 标准化<br>    transfer = StandardScaler()<br>    x_train = transfer.fit_transform(x_train)<br>    x_test = transfer.transform(x_test)<br>    #4 模型<br>    estimator = Ridge(alpha = 0.5,max_iter = 10000)<br>    estimator.fit(x_train,y_train)<br>    #5 评估模型<br>    print(&quot;权重系数为：\n&quot;,estimator.coef_)<br>    print(&quot;偏置为：\n&quot;,estimator.intercept_)<br>    #6 模型评估<br>    y_predict = estimator.predict(x_test)<br>    print(&#x27;预测房价为：\n&#x27;,y_predict)<br>    error = mean_squared_error(y_test,y_predict)<br>    print(&quot;正规方程的均方误差为:\n&quot;,error)<br></code></pre></td></tr></table></figure>
<h2 id="4-4-分类算法-逻辑回归与二分类">4.4 分类算法-逻辑回归与二分类</h2>
<h3 id="4-4-1-逻辑回归的应用场景">4.4.1 逻辑回归的应用场景</h3>
<ul>
<li>广告点击率</li>
<li>是否为垃圾邮件</li>
<li>是否患病</li>
<li>金融诈骗</li>
<li>虚假账号</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">正例 / 反例<br></code></pre></td></tr></table></figure>
<h3 id="4-4-2-逻辑回归原理">4.4.2 逻辑回归原理</h3>
<h3 id="1-输入">1 输入</h3>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_204637.jpg" alt="snipaste20230720_204637"></p>
<p>逻辑回归的输入就是一个线性回归的结果</p>
<h3 id="2-激活函数">2 激活函数</h3>
<ul>
<li>
<p>sigmoid函数 结果代替x</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1/(1 + e^(-x))<br></code></pre></td></tr></table></figure>
</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_204758.jpg" alt=""></p>
<ul>
<li>分析
<ul>
<li>回归的结果输入到sigmoid函数当中</li>
<li>输出结果：[0,1]区间中的一个概率值 默认0.5为阈值</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">假设函数/线性模型<br>    1/(1 + e^(-(w1x1 + w2x2 + w3x3 + …… + wnxn + b)))<br>损失函数<br>    逻辑回归的真实值/预测值 是否属于某个类别<br>    对数似然损失<br>    log 2 x<br>优化损失<br>    梯度下降<br></code></pre></td></tr></table></figure>
<h3 id="3-损失以及优化">3 损失以及优化</h3>
<h4 id="1-优化">1 优化</h4>
<p>逻辑回归的损失 对数似然损失 y为真实值</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_205252.jpg" alt="snipaste20230720_205252"></p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230720_205446.jpg" alt="snipaste20230720_205446"></p>
<h4 id="2-优化">2 优化</h4>
<p>同样使用梯度下降优化算法 去减少损失函数得知 这样去更新逻辑回归前面对应算法的权重参数 提升原本属于1 类别的概率 降低原本是0 的概率</p>
<h3 id="4-4-3-逻辑回归API">4.4.3 逻辑回归API</h3>
<ul>
<li>
<pre><code>sklearn.linear_model.LogisticRegression(solver = 'liblinear',penalty = 'l2',C=1.0)
solver:优化求解方式
sag:根据数据集自动选择 随机平均梯度下降
penalty:正则化的种类
C:正则化力度
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>&gt; 默认将类别数量少的当做正例<br><br>### 4.4.4 案例：癌症分类预测<br><br>- https://archive.ics.uci.edu/ml/machine-learing-databases/<br><br></code></pre></td></tr></table></figure>
恶性 - 正例
      流程分析：
          1）获取数据
              读取的时候加上names
          2）数据处理
              处理缺失值
          3）数据集划分
          4）特征工程：
              无量纲化处理-标准化
          5）逻辑回归预估器
          6）模型评估
</code></pre>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>- 代码：<br><br></code></pre></td></tr></table></figure>
<p>def logisticregression():<br>
“”&quot;<br>
逻辑回归进行癌症预测<br>
:return: None<br>
“”&quot;<br>
# 1、读取数据，处理缺失值以及标准化<br>
column_name = [‘Sample code number’, ‘Clump Thickness’, ‘Uniformity of Cell Size’, ‘Uniformity of Cell Shape’,<br>
‘Marginal Adhesion’, ‘Single Epithelial Cell Size’, ‘Bare Nuclei’, ‘Bland Chromatin’,<br>
‘Normal Nucleoli’, ‘Mitoses’, ‘Class’]</p>
<pre><code>  data = pd.read_csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot;,
                     names=column_name)

  # 删除缺失值
  data = data.replace(to_replace='?', value=np.nan)

  data = data.dropna()

  # 取出特征值
  x = data[column_name[1:10]]

  y = data[column_name[10]]

  # 分割数据集
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)

  # 进行标准化
  std = StandardScaler()

  x_train = std.fit_transform(x_train)

  x_test = std.transform(x_test)

  # 使用逻辑回归
  lr = LogisticRegression()

  lr.fit(x_train, y_train)

  print(&quot;得出来的权重：&quot;, lr.coef_)

  # 预测类别
  print(&quot;预测的类别：&quot;, lr.predict(x_test))

  # 得出准确率
  print(&quot;预测的准确率:&quot;, lr.score(x_test, y_test))
  return None
</code></pre>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>  ### 5 分类的评估方法<br><br>  ### 1 精确率<br><br>  #### 1 混淆矩阵<br><br>  在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)<br><br>  ![snipaste20230721_190811](D:\_xpl2022\Desktop\markdown\picture\snipaste20230721_190811.jpg)<br><br>#### 2 精确率与召回率<br><br>- 精确率：预测结果为正例样本中真实为正例的比例（了解）<br>- 召回率：真实为正例的样本中预测结果为正例的比例（查的全，对正样本的区分能力）<br><br>还有其他的评估标准，F1-score，反映了模型的稳健型 越高越好<br><br>![snipaste20230722_145347](D:\_xpl2022\Desktop\markdown\picture\snipaste20230722_145347.jpg)<br><br>#### 3 分类评估API<br><br>- sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )<br>- - y_true：真实目标值<br>  - y_pred：估计器预测目标值<br>  - labels:指定类别对应的数字 <br>  - target_names：目标类别名称 需要知道真实值和目标值对应的字符串传进来<br>  - return：每个类别精确率与召回率<br><br>`print(&quot;精确率和召回率为：&quot;, classification_report(y_test, lr.predict(x_test), labels=[2, 4], target_names=[&#x27;良性&#x27;, &#x27;恶性&#x27;]))`<br><br>**假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题**<br><br>很不负责任<br><br>准确率：99%<br><br>召回率 99/99<br><br>精确率： 99%<br><br>F1-score：99.497<br><br>#### 问题：如何衡量样本不均衡下的评估？<br><br>### 4.4.5 ROC曲线与AUC指标<br><br>#### 5.2.1 知道TPR与FPR<br><br>- TPR = TP / (TP + FN)-召回率<br>  - 所有真实类别为1的样本中，预测类别为1的比例<br>- FPR = FP / (FP + FN)<br>  - 所有真实类别为0的样本中，预测类别为1的比例<br><br>#### 5.2.2 ROC曲线<br><br>- ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5（曲线和xy轴围成的面积）<br><br>![snipaste20230722_150027](D:\_xpl2022\Desktop\markdown\picture\snipaste20230722_150027.jpg)<br><br>#### 5.2.3AUC指标<br><br>- AUC的概率意义是随机取一对正负样本，正样本得分大于负样本的概率<br>- AUC的最小值为0.5，最大值为1，取值越高越好<br>- **AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。**<br>- **0.5&lt;AUC&lt;1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。**<br><br>&gt; **最终AUC的范围在[0.5, 1]之间，并且越接近1越好**<br><br>#### 5.2.4 AUC计算API<br><br>- from sklearn.metrics import roc_auc_score<br>  - sklearn.metrics.roc_auc_score(y_true, y_score)<br>    - 计算ROC曲线面积，即AUC值<br>    - y_true:每个样本的真实类别，必须为0(反例),1(正例)标记<br>    - y_score:每个样本预测的概率值 预测得分 正类的估计概率 置信值 或者分类器方法的返回值<br><br></code></pre></td></tr></table></figure>
<h1>0.5~1之间，越接近于1约好 因为y_true数值有要求 则大于2.5为1 否则为0</h1>
<p>y_test = np.where(y_test &gt; 2.5, 1,0)<br>
print(“AUC指标：”, roc_auc_score(y_test, lr.predict(x_test)))</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>#### 5.2.5、总结<br><br>- AUC只能用来评价二分类<br>- AUC非常适合评价样本不平衡中的分类器性能<br><br>## 4.5 模型保存和加载<br><br>### 4.5.1 sklearn模型的保存和加载API<br><br>- from sklearn.externals import joblib<br>  - 保存：joblib.dump(rf, &#x27;test.pkl&#x27;)<br>  - 第二个是路径 后缀为pkl<br>  - 加载：estimator = joblib.load(&#x27;test.pkl&#x27;)<br><br>### 4.5.2  线性回归的模型保存加载案例<br><br>- 保存<br><br>```python<br># 使用线性模型进行预测<br># 使用正规方程求解<br>lr = LinearRegression()<br># 此时在干什么？<br>lr.fit(x_train, y_train)<br># 保存训练完结束的模型<br>joblib.dump(lr, &quot;test.pkl&quot;)<br></code></pre></td></tr></table></figure>
<ul>
<li>加载</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 通过已有的模型去预测房价</span><br>model = joblib.load(<span class="hljs-string">&quot;test.pkl&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;从文件加载进来的模型预测房价的结果：&quot;</span>, std_y.inverse_transform(model.predict(x_test)))<br></code></pre></td></tr></table></figure>
<p>说到这个 joblib这个模块自从sklearn升级后给剔除 成为一个独立的包 导入是：</p>
<p><code>import joblib</code></p>
<h2 id="4-6-无监督学习-K-means算法">4.6 无监督学习-K-means算法</h2>
<h3 id="4-6-1-什么是无监督学习">4.6.1 什么是无监督学习</h3>
<p>没有目标值</p>
<ul>
<li>一家广告平台需要根据相似的人口学特征和购买习惯将美国人口分成不同的小组，以便广告客户可以通过有关联的广告接触到他们的目标客户。</li>
<li>Airbnb 需要将自己的房屋清单分组成不同的社区，以便用户能更轻松地查阅这些清单。</li>
<li>一个数据科学团队需要降低一个大型数据集的维度的数量，以便简化建模和降低文件大小。</li>
</ul>
<p>我们可以怎样最有用地对其进行归纳和分组？我们可以怎样以一种压缩格式有效地表征数据？<strong>这都是无监督学习的目标，之所以称之为无监督，是因为这是从无标签的数据开始学习的。</strong></p>
<h3 id="4-6-2-无监督学习包含算法">4.6.2 无监督学习包含算法</h3>
<ul>
<li>聚类
<ul>
<li>K-means(K均值聚类)</li>
</ul>
</li>
<li>降维
<ul>
<li>PCA</li>
</ul>
</li>
</ul>
<h3 id="4-6-3-k-means原理">4.6.3 k-means原理</h3>
<p>我们先来看一下一个K-means的聚类效果图</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230722_152313.jpg" alt="snipaste20230722_152313"></p>
<h3 id="K-means聚类步骤">K-means聚类步骤</h3>
<ul>
<li>1、随机设置K个特征空间内的点作为初始的聚类中心（随机找K个点做中心）</li>
<li>2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</li>
<li>3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</li>
<li>4、如果计算得出的新中心点与原中心点一样，那么结束，否则重新进行第二步过程</li>
</ul>
<p>我们以一张图来解释效果</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230722_152457.jpg" alt="snipaste20230722_152457"></p>
<p>K-超参数</p>
<p>1）看需求</p>
<p>2）网格搜索 调节K值</p>
<h3 id="K-means-API">K-means API</h3>
<ul>
<li><code>sklearn.cluster.KMeans(n_clusters=8,init=‘k-means++’)</code>
<ul>
<li>k-means聚类</li>
<li>n_clusters:开始的聚类中心数量 K的值</li>
<li>init:初始化方法，默认为’k-means ++’</li>
<li>labels_:默认标记的类型，可以和真实值比较（不是值比较）</li>
</ul>
</li>
</ul>
<h3 id="4-6-5-案例：k-means对Instacart-Market用户聚类">4.6.5 案例：k-means对Instacart Market用户聚类</h3>
<h4 id="5-1-分析">5.1 分析</h4>
<ul>
<li>1、降维之后的数据</li>
<li>2、k-means聚类</li>
<li>3、聚类结果显示</li>
</ul>
<h4 id="5-2-代码">5.2 代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 取500个用户进行测试</span><br>cust = data[:<span class="hljs-number">500</span>]<br>km = KMeans(n_clusters=<span class="hljs-number">4</span>)<br>km.fit(cust)<br>pre = km.predict(cust)<br></code></pre></td></tr></table></figure>
<h4 id="问题：如何去评估聚类的效果呢？">问题：如何去评估聚类的效果呢？</h4>
<h3 id="4-6-6-Kmeans-性能评估指标">4.6.6 Kmeans 性能评估指标</h3>
<h4 id="6-1-轮廓系数">6.1 轮廓系数</h4>
<p>高内聚 低耦合</p>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230722_154129.jpg" alt="snipaste20230722_154129"></p>
<blockquote>
<p>注：对于每个点i 为已聚类数据中的样本 ，b_i 为i 到其它族群的所有样本的距离最小值，a_i 为i 到本身簇的距离平均值。最终计算出所有的样本点的轮廓系数平均值</p>
</blockquote>
<h4 id="6-2-轮廓系数值分析">6.2 轮廓系数值分析</h4>
<p><img src="/static/imgs/loading.gif" data-original="D:_xpl2022%5CDesktop%5Cmarkdown%5Cpicture%5Csnipaste20230722_154219.jpg" alt="snipaste20230722_154219"></p>
<ul>
<li>分析过程（我们以一个蓝1点为例）
<ul>
<li>1、计算出蓝1离本身族群所有点的距离的平均值a_i</li>
<li>2、蓝1到其它两个族群的距离计算出平均值红平均，绿平均，取最小的那个距离作为b_i</li>
<li>根据公式：极端值考虑：如果b_i &gt;&gt;a_i: 那么公式结果趋近于1；如果a_i&gt;&gt;&gt;b_i: 那么公式结果趋近于-1</li>
</ul>
</li>
</ul>
<h4 id="6-3-结论">6.3 结论</h4>
<p><strong>如果b_i&gt;&gt;a_i:趋近于1效果越好， b_i&lt;&lt;a_i:趋近于-1，效果不好。轮廓系数的值是介于 [-1,1] ，越趋近于1代表内聚度和分离度都相对较优。</strong></p>
<h4 id="6-4-轮廓系数-API">6.4 轮廓系数 API</h4>
<ul>
<li>sklearn.metrics.silhouette_score(X, labels)
<ul>
<li>计算所有样本的平均轮廓系数</li>
<li>X：特征值</li>
<li>labels：被聚类标记的目标值</li>
</ul>
</li>
</ul>
<h4 id="6-5-用户聚类结果评估">6.5 用户聚类结果评估</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">silhouette_score(cust, pre)<br>cust-data_new<br>pre - y_predict<br></code></pre></td></tr></table></figure>
<h4 id="7、K-means总结">7、K-means总结</h4>
<ul>
<li>特点分析：采用迭代式算法，直观易懂并且非常实用</li>
<li>缺点：容易收敛到局部最优解(多次聚类)</li>
</ul>
<blockquote>
<p>注意：聚类一般做在分类之前</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://gxblogs.com">ggw &amp; xpl</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://gxblogs.com/posts/498ab7d9/">https://gxblogs.com/posts/498ab7d9/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://gxblogs.com" target="_blank">GXBLOGS</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_112943.jpg" data-sites="wechat,qq"></div><link rel="stylesheet" href="/pluginsSrc/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/pluginsSrc/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-03-06.png" target="_blank"><img class="post-qr-code-img" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-03-06.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-04-50.png" target="_blank"><img class="post-qr-code-img" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-04-50.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/60728/"><img class="prev-cover" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-08-31_08-29-31.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">408做题笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/19471/"><img class="next-cover" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-08-19_10-13-57.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数一</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/11163/" title="人工智能（东软）"><img class="cover" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/python/ai2023learning/Snipaste_2023-02-20_11-44-21.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-20</div><div class="title">人工智能（东软）</div></div></a></div><div><a href="/posts/48513/" title="深度学习"><img class="cover" src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ggw2021/images@main/blogs/Snipaste_2024-01-24_00-26-26.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-23</div><div class="title">深度学习</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">三天入门机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%A6%82%E8%BF%B0"><span class="toc-text">1.1 人工智能概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.1.1 机器学习与人工智能 深度学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%81%9A%E4%BA%9B%E4%BB%80%E4%B9%88"><span class="toc-text">1.1.2机器学习、深度学习能做些什么</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.2什么是机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-%E5%AE%9A%E4%B9%89"><span class="toc-text">1.2.1 定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-3-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E6%88%90"><span class="toc-text">1.2.3 数据集构成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-text">1.3 机器学习算法分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">1.3.1 监督学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-text">1.4 机器学习开发流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E5%92%8C%E8%B5%84%E6%96%99%E4%BB%8B%E7%BB%8D"><span class="toc-text">1.5 学习框架和资料介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93%E4%B8%8E%E6%A1%86%E6%9E%B6"><span class="toc-text">1.5.1 机器学习库与框架</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.1 数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1%E5%8F%AF%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.1.1可用数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scikit-learn%E5%B7%A5%E5%85%B7%E4%BB%8B%E7%BB%8D"><span class="toc-text">Scikit-learn工具介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-sklearn-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.1.2 sklearn 数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-scikit-learn-%E6%95%B0%E6%8D%AE%E9%9B%86api%E4%BB%8B%E7%BB%8D"><span class="toc-text">1 scikit-learn 数据集api介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-sklearn-%E5%B0%8F%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2 sklearn 小数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-sklearn%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">3 sklearn大数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-sklearn%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">4 sklearn数据集的使用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86"><span class="toc-text">2.1.3 数据集的划分</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%BB%8B%E7%BB%8D"><span class="toc-text">2.2 特征工程介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81"><span class="toc-text">2.2.1 为什么需要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">2.2.2 什么是特征工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-1-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-text">2.3.1 特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%B0%86%E4%BB%BB%E6%84%8F%E6%95%B0%E6%8D%AE%EF%BC%88%E6%96%87%E6%9C%AC%E3%80%81%E5%9B%BE%E5%83%8F%EF%BC%89%E8%BD%AC%E6%8D%A2%E4%B8%BA%E5%8F%AF%E7%94%A8%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%97%E7%89%B9%E5%BE%81"><span class="toc-text">1 将任意数据（文本、图像）转换为可用于机器学习的数字特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96API"><span class="toc-text">2 特征提取API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-2-%E5%AD%97%E5%85%B8%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-%E7%B1%BB%E5%88%AB-one-hot%E7%BC%96%E7%A0%81"><span class="toc-text">2.3.2 字典特征提取 -类别-&gt;one-hot编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-3-%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-text">2.3.3 文本特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%951%EF%BC%9ACountVectorizer"><span class="toc-text">方法1：CountVectorizer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%952%EF%BC%9A-jieba%E8%BF%99%E4%B8%80%E8%87%AA%E5%8A%A8%E5%88%86%E8%8A%82%E5%99%A8"><span class="toc-text">方法2： jieba这一自动分节器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%953%EF%BC%9A-TfidfVectorizer"><span class="toc-text">方法3： TfidfVectorizer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#API"><span class="toc-text">API</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">2.4 特征预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-1-%E5%AE%9A%E4%B9%89"><span class="toc-text">2.4.1 定义</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8C%85%E5%90%AB%E5%86%85%E5%AE%B9"><span class="toc-text">1 包含内容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86API"><span class="toc-text">2 特征预处理API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-2-%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">2.4.2 归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89"><span class="toc-text">1 定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-API"><span class="toc-text">2 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97"><span class="toc-text">3 数据计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-3-%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">2.4.3 标准化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89-2"><span class="toc-text">1 定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%85%AC%E5%BC%8F"><span class="toc-text">2 公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-API"><span class="toc-text">3 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97"><span class="toc-text">4 数据计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4"><span class="toc-text">2.5 特征降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-1-%E9%99%8D%E7%BB%B4"><span class="toc-text">2.5.1 降维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-2-%E9%99%8D%E7%BB%B4%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F"><span class="toc-text">2.5.2 降维的两种方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-3-%E4%BB%80%E4%B9%88%E6%98%AF%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-text">2.5.3 什么是特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89-3"><span class="toc-text">1 定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%96%B9%E6%B3%95"><span class="toc-text">2 方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%A8%A1%E5%9D%97"><span class="toc-text">3 模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%BF%87%E6%BB%A4%E5%BC%8F"><span class="toc-text">4 过滤式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-%E4%BD%8E%E6%96%B9%E5%B7%AE%E7%89%B9%E5%BE%81%E8%BF%87%E6%BB%A4"><span class="toc-text">4.1 低方差特征过滤</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4-1-1-API"><span class="toc-text">4.1.1 API</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-1-2-%E5%AE%9E%E4%BE%8B"><span class="toc-text">4.1.2 实例</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0"><span class="toc-text">4.2 相关系数</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#4-2-3-%E7%89%B9%E7%82%B9"><span class="toc-text">4.2.3 特点</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-2-4-API"><span class="toc-text">4.2.4 API</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#4-2-5-%E8%82%A1%E7%A5%A8%E6%A1%88%E4%BE%8B"><span class="toc-text">4.2.5 股票案例</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90"><span class="toc-text">2.6 主成分分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89"><span class="toc-text">2.6.1 什么是主成分分析（PCA）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%AE%A1%E7%AE%97%E6%A1%88%E4%BE%8B"><span class="toc-text">1 计算案例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-API-2"><span class="toc-text">2 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E8%AE%A1%E7%AE%97-2"><span class="toc-text">3 数据计算</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-2-%E6%8E%A2%E7%A9%B6%E7%94%A8%E6%88%B7%E5%AF%B9%E7%89%A9%E5%93%81%E7%B1%BB%E5%88%AB%E7%9A%84%E5%96%9C%E5%A5%BD%E7%BB%86%E5%88%86"><span class="toc-text">2.6.2 探究用户对物品类别的喜好细分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%9C%80%E6%B1%82"><span class="toc-text">1 需求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%86%E6%9E%90"><span class="toc-text">2 分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-sklearn%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%92%8C%E4%BC%B0%E8%AE%A1%E5%99%A8"><span class="toc-text">3.1 sklearn转换器和估计器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E8%BD%AC%E6%8D%A2%E5%99%A8"><span class="toc-text">3.1.1 转换器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E4%BC%B0%E8%AE%A1%E5%99%A8estimator"><span class="toc-text">3.1.2 估计器estimator</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95-KNN"><span class="toc-text">3.2 K-近邻算法 KNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E4%BB%80%E4%B9%88%E6%98%AFKNN"><span class="toc-text">3.2.1 什么是KNN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8E%9F%E7%90%86"><span class="toc-text">1 原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%94%B5%E5%BD%B1%E7%B1%BB%E5%9E%8B%E5%88%86%E6%9E%90"><span class="toc-text">2 电影类型分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%97%AE%E9%A2%98"><span class="toc-text">3 问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-KNN-API"><span class="toc-text">3.2.2 KNN API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E6%A1%88%E4%BE%8B"><span class="toc-text">3.2.3 案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-text">1 数据集介绍</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98"><span class="toc-text">3.3 模型选择与调优</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-text">3.3.1 什么是交叉验证</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%88%86%E6%9E%90"><span class="toc-text">1 分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%9B%AE%E7%9A%84"><span class="toc-text">2 目的</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2-%E7%BD%91%E6%A0%BC%E6%90%9C%E7%B4%A2%EF%BC%88Grid-Search%EF%BC%89"><span class="toc-text">3.3.2 超参数搜索-网格搜索（Grid-Search）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%B0%83%E4%BC%98API"><span class="toc-text">模型选择与调优API</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%94%B9%E8%BF%9B"><span class="toc-text">3.3.3 鸢尾花改进</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-%E9%A2%84%E6%B5%8Bfacebook%E7%AD%BE%E5%88%B0%E4%BD%8D%E7%BD%AE"><span class="toc-text">3.3.4 预测facebook签到位置</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D-2"><span class="toc-text">1 数据集介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AE%9E%E9%99%85%E6%93%8D%E4%BD%9C"><span class="toc-text">2 实际操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95"><span class="toc-text">3.4 朴素贝叶斯算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95"><span class="toc-text">3.4.1 什么是朴素贝叶斯算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80"><span class="toc-text">3.4.2 概率基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87-%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87-%E7%9B%B8%E4%BA%92%E7%8B%AC%E7%AB%8B"><span class="toc-text">3.4.3 联合概率 条件概率 相互独立</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-4-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="toc-text">3.4.4 贝叶斯公式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%85%AC%E5%BC%8F"><span class="toc-text">1 公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%96%87%E7%AB%A0%E5%88%86%E7%B1%BB%E8%AE%A1%E7%AE%97"><span class="toc-text">2 文章分类计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91%E7%B3%BB%E6%95%B0"><span class="toc-text">3 拉普拉斯平滑系数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-5-API"><span class="toc-text">3.4.5 API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-6-%E6%A1%88%E4%BE%8B%EF%BC%9A20%E4%B8%AA%E6%96%B0%E9%97%BB%E7%BB%84"><span class="toc-text">3.4.6 案例：20个新闻组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-7-%E6%80%BB%E7%BB%93"><span class="toc-text">3.4.7 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-text">3.5 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1-%E8%AE%A4%E8%AF%86%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-text">3.5.1 认识决策树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2-%E5%86%B3%E7%AD%96%E6%A0%91%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3"><span class="toc-text">3.5.2 决策树原理详解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8E%9F%E7%90%86-2"><span class="toc-text">1 原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BF%A1%E6%81%AF%E7%86%B5%E7%9A%84%E5%AE%9A%E4%B9%89-%E4%BF%A1%E6%81%AF%E7%9A%84%E8%A1%A1%E9%87%8F-%E4%BF%A1%E6%81%AF%E9%87%8F-%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="toc-text">2 信息熵的定义 信息的衡量 - 信息量 - 信息熵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%88%92%E5%88%86%E4%BE%9D%E6%8D%AE%E4%B9%8B%E4%B8%80%E2%80%94%E2%80%94%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-text">3 决策树的划分依据之一——信息增益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%85%B6%E4%BB%96%E4%BE%9D%E6%8D%AE"><span class="toc-text">4 其他依据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-3-%E5%86%B3%E7%AD%96%E6%A0%91API"><span class="toc-text">3.5.3 决策树API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-4-%E6%A1%88%E4%BE%8B%EF%BC%9A%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B"><span class="toc-text">3.5.4 案例：泰坦尼克号乘客生存预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B9%98%E5%9D%90%E7%8F%AD%E6%98%AF%E6%8C%87%E4%B9%98%E5%AE%A2%E7%8F%AD%EF%BC%881-2-3%EF%BC%89-%E7%A4%BE%E4%BC%9A%E7%BB%8F%E6%B5%8E%E9%98%B6%E5%B1%82%E7%9A%84%E4%BB%A3%E8%A1%A8"><span class="toc-text">1 乘坐班是指乘客班（1,2,3） 社会经济阶层的代表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%85%B6%E4%B8%ADage%E6%95%B0%E6%8D%AE%E5%AD%98%E5%9C%A8%E7%BC%BA%E5%A4%B1"><span class="toc-text">2 其中age数据存在缺失</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E6%AD%A5%E9%AA%A4"><span class="toc-text">分析步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-3-%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">3.5.3 决策树可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BF%9D%E5%AD%98%E6%A0%91%E7%9A%84%E7%BB%93%E6%9E%84%E5%88%B0dot%E6%96%87%E4%BB%B6"><span class="toc-text">1 保存树的结构到dot文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%BD%91%E7%AB%99%E7%BB%93%E6%9E%84"><span class="toc-text">2 网站结构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-6-%E6%80%BB%E7%BB%93"><span class="toc-text">3.5.6 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-text">3.6 集成学习方法之随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-text">3.6.1 什么是集成学习方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-2-%E4%BB%80%E4%B9%88%E6%98%AF%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-text">3.6.2 什么是随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-3-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%8E%9F%E7%90%86%E8%BF%87%E7%A8%8B"><span class="toc-text">3.6.3 随机森林原理过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-4-API"><span class="toc-text">3.6.4 API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-5-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E9%A2%84%E6%B5%8B%E6%A1%88%E4%BE%8B"><span class="toc-text">3.6.5 随机森林预测案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-6-%E6%80%BB%E7%BB%93"><span class="toc-text">3.6.6 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%9B%9E%E5%BD%92%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-text">4 回归与聚类算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">4.1线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-text">4.1.1 线性回归的内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BB%80%E4%B9%88%E6%98%AF%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">2 什么是线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%92%8C%E4%BC%98%E5%8C%96%E5%8E%9F%E7%90%86%EF%BC%88%E7%90%86%E8%A7%A3%E8%AE%B0%E5%BF%86%EF%BC%89"><span class="toc-text">4.1.2 线性回归的损失和优化原理（理解记忆）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">1 损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text">2 优化算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92API"><span class="toc-text">4.1.3 线性回归API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-4-%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-text">4.1.4 波士顿房价预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%9B%9E%E5%BD%92%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0"><span class="toc-text">1 回归性能评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%AF%B9%E6%AF%94"><span class="toc-text">2 对比</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">4.2 欠拟合和过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E4%BB%80%E4%B9%88%E6%98%AF%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">4.2.1 什么是过拟合与欠拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E5%8E%9F%E5%9B%A0%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="toc-text">4.2.2 原因以及解决办法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">1 正则化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-text">4.3 岭回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E5%B8%A6%E6%9C%89L2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E2%80%94%E2%80%94%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-text">4.3.1 带有L2正则化的线性回归——岭回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-API"><span class="toc-text">1 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%A7%82%E5%AF%9F%E6%AD%A3%E5%88%99%E5%8C%96%E7%A8%8B%E5%BA%A6%E7%9A%84%E5%8F%98%E5%8C%96-%E5%AF%B9%E7%BB%93%E6%9E%9C%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">2 观察正则化程度的变化 对结果的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%B3%A2%E5%A3%AB%E9%A1%BF%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-text">3 波士顿房价预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="toc-text">4.4 分类算法-逻辑回归与二分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">4.4.1 逻辑回归的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%8E%9F%E7%90%86"><span class="toc-text">4.4.2 逻辑回归原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%BE%93%E5%85%A5"><span class="toc-text">1 输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">2 激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96"><span class="toc-text">3 损失以及优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BC%98%E5%8C%96"><span class="toc-text">1 优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BC%98%E5%8C%96"><span class="toc-text">2 优化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92API"><span class="toc-text">4.4.3 逻辑回归API</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">0.5~1之间，越接近于1约好 因为y_true数值有要求 则大于2.5为1 否则为0</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-K-means%E7%AE%97%E6%B3%95"><span class="toc-text">4.6 无监督学习-K-means算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">4.6.1 什么是无监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%8C%85%E5%90%AB%E7%AE%97%E6%B3%95"><span class="toc-text">4.6.2 无监督学习包含算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-3-k-means%E5%8E%9F%E7%90%86"><span class="toc-text">4.6.3 k-means原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means%E8%81%9A%E7%B1%BB%E6%AD%A5%E9%AA%A4"><span class="toc-text">K-means聚类步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-means-API"><span class="toc-text">K-means API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-5-%E6%A1%88%E4%BE%8B%EF%BC%9Ak-means%E5%AF%B9Instacart-Market%E7%94%A8%E6%88%B7%E8%81%9A%E7%B1%BB"><span class="toc-text">4.6.5 案例：k-means对Instacart Market用户聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-%E5%88%86%E6%9E%90"><span class="toc-text">5.1 分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-%E4%BB%A3%E7%A0%81"><span class="toc-text">5.2 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%EF%BC%9A%E5%A6%82%E4%BD%95%E5%8E%BB%E8%AF%84%E4%BC%B0%E8%81%9A%E7%B1%BB%E7%9A%84%E6%95%88%E6%9E%9C%E5%91%A2%EF%BC%9F"><span class="toc-text">问题：如何去评估聚类的效果呢？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-6-Kmeans-%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">4.6.6 Kmeans 性能评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-1-%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0"><span class="toc-text">6.1 轮廓系数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-2-%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90"><span class="toc-text">6.2 轮廓系数值分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-3-%E7%BB%93%E8%AE%BA"><span class="toc-text">6.3 结论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-%E8%BD%AE%E5%BB%93%E7%B3%BB%E6%95%B0-API"><span class="toc-text">6.4 轮廓系数 API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-5-%E7%94%A8%E6%88%B7%E8%81%9A%E7%B1%BB%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BC%B0"><span class="toc-text">6.5 用户聚类结果评估</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7%E3%80%81K-means%E6%80%BB%E7%BB%93"><span class="toc-text">7、K-means总结</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: rgb(112, 112, 112)"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By ggw & xpl</div><div class="footer_custom_text">都是科技与狠活啊</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/pluginsSrc/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/pluginsSrc/instant.page/instantpage.js" type="module"></script><script src="/pluginsSrc/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="/pluginsSrc/katex/dist/katex.min.css"><script src="/pluginsSrc/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'QvVqZWBQWj9Aq2xrw4Z8z8Pz-gzGzoHsz',
      appKey: 'COANudzj20V6IrCfAsD1Ufya',
      avatar: 'robohash',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('/pluginsSrc/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/js/ggwsettings.js"></script><script>(function(d, w, c) {
    w.ChatraID = 'dKkRxvMac7f9AeKhu';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>