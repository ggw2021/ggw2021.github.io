<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习 | GXBLOGS</title><meta name="author" content="ggw &amp; xpl"><meta name="copyright" content="ggw &amp; xpl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="三天入门深度学习 123456789101112TensorFlow框架使用 1天数据读取、神经网络基础 1天卷积神经网络、验证码识别 1天1、深度学习介绍2、TensorFlow框架的使用    1）TensorFlow的结构    2）TensorFlow的各个组件        图        会话        张量        变量    3）简单的线性回归案例 - 将TensorF">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习">
<meta property="og:url" content="https://gxblogs.com/posts/48513/index.html">
<meta property="og:site_name" content="GXBLOGS">
<meta property="og:description" content="三天入门深度学习 123456789101112TensorFlow框架使用 1天数据读取、神经网络基础 1天卷积神经网络、验证码识别 1天1、深度学习介绍2、TensorFlow框架的使用    1）TensorFlow的结构    2）TensorFlow的各个组件        图        会话        张量        变量    3）简单的线性回归案例 - 将TensorF">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ggw2021/images@main/blogs/Snipaste_2024-01-24_00-26-26.png">
<meta property="article:published_time" content="2024-01-23T10:44:44.667Z">
<meta property="article:modified_time" content="2024-01-24T12:25:24.180Z">
<meta property="article:author" content="ggw &amp; xpl">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/ggw2021/images@main/blogs/Snipaste_2024-01-24_00-26-26.png"><link rel="shortcut icon" href="/static/imgs/paipai.ico"><link rel="canonical" href="https://gxblogs.com/posts/48513/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: ggw & xpl","link":"链接: ","source":"来源: GXBLOGS","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-24 20:25:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/ggwsettings.css"><link rel="stylesheet" href="/css/nav_menu.css"><link rel="stylesheet" href="/css/highlight/Kimbiedark.css"><script src="/css/else/echarts.min.js"></script><script src="/css/else/wow.min.js"></script><script type="text/javascript" src="/css/else/echarts-gl.min.js"></script><script src="/css/else/pace.min.js"></script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/static/imgs/loading.gif" data-original="/static/imgs/touxiang.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-comments"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/ggw2021/images@main/blogs/Snipaste_2024-01-24_00-26-26.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">GXBLOGS</a></span><div id="menus"></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 文章</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-comments"></i><span> 留言板</span></a></div></div><div id="nav-right"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-23T10:44:44.667Z" title="发表于 2024-01-23 18:44:44">2024-01-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-24T12:25:24.180Z" title="更新于 2024-01-24 20:25:24">2024-01-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">14.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>57分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/posts/48513/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/posts/48513/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>三天入门深度学习</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">TensorFlow框架使用 1天<br>数据读取、神经网络基础 1天<br>卷积神经网络、验证码识别 1天<br>1、深度学习介绍<br>2、TensorFlow框架的使用<br>    1）TensorFlow的结构<br>    2）TensorFlow的各个组件<br>        图<br>        会话<br>        张量<br>        变量<br>    3）简单的线性回归案例 - 将TensorFlow用起来<br></code></pre></td></tr></table></figure>
<h2 id="1-1-深度学习与机器学习的区别">1.1 深度学习与机器学习的区别</h2>
<h3 id="1-1-1-特征提取方面">1.1.1 特征提取方面</h3>
<ul>
<li>机器学习的<strong>特征工程步骤是需要靠手动完成的 需要大量领域专业知识</strong></li>
<li>深度学习<strong>通常由多个层组成 通常将更简单的模型组合在一起 将数据从一层传递到另一层来构建更复杂的模型  通过训练大量的数据自动得出的模型 不需要人工特征提取环节</strong></li>
</ul>
<blockquote>
<p>深度学习算法试图从数据中学习高级功能，这是深度学习的一个非常独特的部分 因此 减少了每个问题开发新特征提取器的任务。适合用在难提取特征的图像、语音、自然语言处理领域。</p>
</blockquote>
<h3 id="1-1-2-数据量和计算性能要求">1.1.2 数据量和计算性能要求</h3>
<p>机器学习需要的执行时间远少于深度学习 深度学习参数往往很庞大 需要通过大量数据的多次优化来训练参数。</p>
<blockquote>
<p>第一，深度学习需要大量的训练数据集</p>
<p>第二，训练深度神经网络需要大量的算力</p>
<p>可能需要花费数天，数周 才能训练出一个深度网络</p>
<ul>
<li>需要强大的GPU</li>
<li>全面管理的分布式训练与预测服务——比如谷歌tensorflow云机器学习平台</li>
</ul>
</blockquote>
<h3 id="1-1-3-算法代表">1.1.3 算法代表</h3>
<ul>
<li>机器学习
<ul>
<li>朴素贝叶斯 决策树</li>
</ul>
</li>
<li>深度学习
<ul>
<li>神经网络</li>
</ul>
</li>
</ul>
<h2 id="1-2-深度学习的应用场景">1.2 深度学习的应用场景</h2>
<ul>
<li>
<p>图像识别</p>
<ul>
<li>物体识别</li>
<li>场景识别</li>
<li>车型识别</li>
<li>人脸检测跟踪</li>
<li>人脸关键点定位</li>
<li>人脸身份认证</li>
</ul>
</li>
<li>
<p>自然语言处理技术</p>
<ul>
<li>机器翻译</li>
<li>文本识别</li>
<li>聊天对话</li>
</ul>
</li>
<li>
<p>语音技术</p>
<ul>
<li>语音识别</li>
</ul>
</li>
</ul>
<h2 id="1-3-深度学习框架介绍">1.3 深度学习框架介绍</h2>
<h3 id="1-3-1-常见深度学习框架对比">1.3.1 常见深度学习框架对比</h3>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230722_162340.jpg" alt="snipaste20230722_162340"></p>
<h3 id="1-3-2-TensorFlow-的特点">1.3.2 TensorFlow 的特点</h3>
<p>官网：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></p>
<ul>
<li>高度灵活</li>
<li>语言多样</li>
<li>设备支持</li>
<li>Tensorboard可视化</li>
</ul>
<h3 id="1-3-3-Tensorflow的安装">1.3.3 Tensorflow的安装</h3>
<h3 id="1-CPU版本">1 CPU版本</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">CPU:诸葛亮<br>    综合能力比较强<br>    核芯的数量更少<br>    更适用于处理连续性（sequential）任务。<br>GPU:臭皮匠<br>    专做某一个事情很好<br>    核芯的数量更多<br>    更适用于并行（parallel）任务<br></code></pre></td></tr></table></figure>
<h3 id="2-GPU版本">2 GPU版本</h3>
<ul>
<li>很奇怪能用 但是后面的课程再看看</li>
</ul>
<h2 id="2-2-TF数据流图">2.2 TF数据流图</h2>
<h3 id="2-2-1-案例：Tensorflow实现一个加法运算">2.2.1 案例：Tensorflow实现一个加法运算</h3>
<h4 id="1-代码">1 代码</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import tensorflow as tf<br>def tensorflow_demo():<br>    #Tensorflow实现加法运算 图<br>    a_t = tf.constant(2)<br>    b_t = tf.constant(3)<br>    c_t = a_t+b_t<br>    print(&quot;结果为:\n&quot;,c_t)<br>    return None<br>if __name__ == &quot;__main__&quot;:<br>    tensorflow_demo()<br></code></pre></td></tr></table></figure>
<h4 id="结果：">结果：</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">#开启会话 这样才能看到值 新版中已经没有这个功能了<br>with tf.Session() as sess:<br>c_t_value = sess.run(c_t)<br>print(&#x27;c_t_value:\n&#x27;,c_t_value)<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">import tensorflow._api.v2.compat.v1 as tf<br>tf.compat.v1.disable_eager_execution()<br>#变成这两句话 可以使用之前版本的语法 但得考虑一个问题就是后面神经网络有可能有问题<br></code></pre></td></tr></table></figure>
<h4 id="2-Tensorflow-结构分析">2 Tensorflow 结构分析</h4>
<p>一个构建图的阶段 一个执行图的阶段</p>
<p>在构建阶段 数据与操作的执行步骤被描述成一个图</p>
<p>执行阶段 使用会话执行构建好图中的操作</p>
<ul>
<li>图和会话：
<ul>
<li>图：这是Tensorflow将计算表示为指令之间的依赖关系的一种表示法 定义数据（张量）和操作</li>
<li>会话:  Tensorflow 跨一个或多个本地或远程设备运行数据流图的机制</li>
</ul>
</li>
<li>张量：Tensorflow中的基本数据对象</li>
<li>节点（op）：提供图当中执行的操作</li>
</ul>
<h3 id="2-1-2-数据流图介绍">2.1.2 数据流图介绍</h3>
<h3 id="2-2-1-什么是图结构">2.2.1 什么是图结构</h3>
<p>图包含了一组tf.Operation代表的计算单元对象和tf.Tensor代表的计算单元之间流动的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">图结构：<br>    数据（Tensor） + 操作（Operation）<br></code></pre></td></tr></table></figure>
<h3 id="2-2-2-图相关操作">2.2.2 图相关操作</h3>
<h4 id="1-默认图">1 默认图</h4>
<p>通常Tensorflow会默认创建一张图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1 默认图<br>    查看默认图的方法<br>        1）调用方法<br>            用tf.get_default_graph()<br>        2）查看属性<br>            .graph<br></code></pre></td></tr></table></figure>
<p>查看默认图的两种方法：</p>
<ul>
<li>通过调用tf.get_default_graph()访问 要将操作添加到默认图形中 直接创建OP即可</li>
<li>op/sess都有graph属性 默认都在一张图中</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def graph_demo():<br>    a_t = tf.constant(2)<br>    b_t = tf.constant(3)<br>    c_t = a_t + b_t<br>    print(&quot;TensorFlow加法运算的结果：\n&quot;, c_t)<br>    #查看默认图<br>    #1）方法一：调用方法<br>    default_g = tf.get_default_graph()<br>    print(&quot;default_g:\n&quot;,default_g)<br>    # 1）方法二：调用属性<br>    print(&quot;a_t的属性:\n&quot;,a_t.graph)<br>    print(&quot;c_t的属性:\n&quot;, c_t.graph)<br>    with tf.Session() as sess:<br>        c_t_value = sess.run(c_t)<br>        print(&quot;c_t_value:\n&quot;, c_t_value)<br>        print(&quot;sess_t的属性:\n&quot;, sess.graph)<br>    return None<br>    #打印出来的都是图的地址<br></code></pre></td></tr></table></figure>
<h4 id="2-创建图">2 创建图</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">new_g = tf.Graph()<br>with new_g.as_default():<br>    定义数据和操作<br></code></pre></td></tr></table></figure>
<ul>
<li>通过tf.Graph()</li>
<li>如果要在这张图中创建OP tf.Graph.as_default()</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def graph_demo():<br>    a_t = tf.constant(2)<br>    b_t = tf.constant(3)<br>    c_t = a_t + b_t<br>    print(&quot;TensorFlow加法运算的结果：\n&quot;, c_t)<br>    #查看默认图<br>    #1）方法一：调用方法<br>    default_g = tf.get_default_graph()<br>    print(&quot;default_g:\n&quot;,default_g)<br>    # 1）方法二：调用属性<br>    print(&quot;a_t的属性:\n&quot;,a_t.graph)<br>    print(&quot;c_t的属性:\n&quot;, c_t.graph)<br>    with tf.Session() as sess:<br>        c_t_value = sess.run(c_t)<br>        print(&quot;c_t_value:\n&quot;, c_t_value)<br>        print(&quot;sess_t的属性:\n&quot;, sess.graph)<br>        # 自定义图<br>    new_g = tf.Graph()<br>    with new_g.as_default():<br>        a_new = tf.constant(20)<br>        b_new = tf.constant(30)<br>        c_new = a_new + b_new<br>        print(&quot;c_new：\n&quot;, c_new)<br>        print(&quot;a_new的图属性：\n&quot;, a_new.graph)<br>        print(&quot;c_new的图属性：\n&quot;, c_new.graph)<br>        #开启新的会话<br>    with tf.Session(graph=new_g) as new_sess:<br>        c_new_value = new_sess.run(c_new)<br>        print(&quot;c_new_value:\n&quot;, c_new_value)<br>        print(&quot;new_sess的图属性：\n&quot;, new_sess.graph)<br>    return None<br></code></pre></td></tr></table></figure>
<p>Tensorflow可以看到写的程序可视化效果 就是Tensorboard</p>
<h3 id="2-2-3-TensorBoard-可视化学习">2.2.3 TensorBoard:可视化学习</h3>
<p>Tensorflow可用于训练大规模深度神经网络所需的计算 使用往往涉及的计算复杂而深奥 为了更方便Tensorflow程序的理解 调试和优化 提供了可视化工具</p>
<h4 id="1-数据序列化-events文件">1 数据序列化-events文件</h4>
<p>Tensorboard 通过读取Tensorflow的事件文件来运行 需要将数据生成一个序列化的Summary protobuf对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">#返回filewriter 写入事件文件到指定目录（最好用绝对路径），提供给tensorboard使用<br>tf.summary.FileWriter(&#x27;./././&#x27;,graph = sess.graph)<br></code></pre></td></tr></table></figure>
<p>这将在指定目录中生成一个event文件 名称格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">events.out.tfevents.&#123;timestamp&#125;.&#123;hostname&#125;<br></code></pre></td></tr></table></figure>
<h4 id="2-启动TensorBoard">2 启动TensorBoard</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tensorboard --logdir = &quot;./././&quot;<br></code></pre></td></tr></table></figure>
<h3 id="2-2-4-OP">2.2.4 OP</h3>
<h3 id="1-常见OP">1 常见OP</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">数据：Tensor对象<br>操作：Operation对象 - Op<br>1 常见OP<br>    操作函数        &amp;                           操作对象<br>    tf.constant(Tensor对象)           输入Tensor对象 -Const-输出 Tensor对象<br>    tf.add(Tensor对象1, Tensor对象2)   输入Tensor对象1, Tensor对象2 - Add对象 - 输出 Tensor对象3<br>2 指令名称<br>    一张图 - 一个命名空间<br></code></pre></td></tr></table></figure>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230725_160509.jpg" alt=""></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">c = tf.matmul(a,b) <br>#创建了一个operation对象 类型为MatMul 将张量a,b输入 c作为输出 打印的时候也是打印数据<br>#其中tf.matmul()是函数 通过这个类创建一个和它对应的对象<br></code></pre></td></tr></table></figure>
<p>注意，打印出来的是张量值 理解成OP中包含了这个值 每一个OP指令都对应一个惟一的名称 如上面的Const:0</p>
<p>注意 tf.Tensor对象是以输出该张量的tf.Operation明确命名</p>
<p>张量名称的形式&quot; &lt;OP_NAME&gt;:<I>&quot; 其中：</p>
<ul>
<li>前面这个OP_NAME是生成该张量的指令名称</li>
<li>后面这个I 是这个张量在指令输出中的索引</li>
</ul>
<h3 id="2-指令名称">2 指令名称</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">一张图 - 一个命名空间<br></code></pre></td></tr></table></figure>
<p>tf.Graph对象为其包含的tf.Operation对象定义的一个命名空间。Tensorflow 会自动为图中的每个指令选择一个唯一名称 用户也可以指定描述性名称</p>
<p>改写指令名称：</p>
<ul>
<li>每个创建的tf.Operation或返回新的tf.Tensor的API函数可以接受可选的name参数</li>
</ul>
<p>例如：tf.Operation(42.0,name=“answer”)创建了一个名为”answer&quot;的新tf.Operation并返回一个名为“answer:0&quot;的tf.Tensor. 如果默认图已包含名为”answer“的指令 则Tensorflow 会在名称上加”1“”2“等字符 让名称具有唯一性</p>
<ul>
<li>
<pre><code>a = tf.constant(3.0,name=&quot;a&quot;)
b = tf.constant(3.0,name=&quot;b&quot;)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>## 2.3 会话<br><br>### 2.3.1 会话<br><br>一个运行Tensorflow operation的类 会话包括以下两种开启方式<br><br>- tf.Session: 用于完整程序当中<br><br>- tf.InteractiveSession:用于交互式上下文中的Tensorflow 例如shell<br><br>&gt; 1 Tensorflow 使用tf.Session类表示客户端程序（通常为python）与C++运行时的连接<br>&gt;<br>&gt; 2 tf.Session对象使用分布式Tensorflow运行时提供对本地计算机中的设备和远程设备的访问权限<br><br>### 1 `_init_(target=&#x27;&#x27;,graph = None,config= None)`<br><br>开启终端 找到views-&gt;tools-&gt;terminal 可以直接进入<br><br>也可以是使用activate.bat那个路径从cmd直接进入<br><br>会话可能拥有的资源 如tf.Variable, tf.QueueBase 和tf.ReaderBase 当这些资源不再需要时 释放这些资源很重要 因此 需要调用tf.Session.close会话中的方法 或将会话用作上下文管理器 <br><br>把c_t_value 经过session变换的值直接变成`c_t.eval()`<br><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p>def session_demo():<br>
“”&quot;<br>
会话的演示<br>
:return:<br>
“”&quot;</p>
<pre><code># TensorFlow实现加法运算
a_t = tf.constant(10)
b_t = tf.constant(20)
# 不提倡直接使用这种符号运算进行计算
# 更经常使用tensorflow提供的函数进行计算
c_t = tf.add(a_t, b_t)
print(&quot;a_t:\n&quot;, a_t)
print(&quot;b_t:\n&quot;, b_t)
print(&quot;c_t:\n&quot;, c_t)
print(&quot;c_t.eval():\n&quot;, c_t.eval()
# 开启会话
with tf.Session() as sess:
   # sum_t = sess.run(c_t)
   #想同时执行多个tensor
   print(sess.run([a_t,b_t,c_t]))
   #方便获取张量的方法
return None
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br></code></pre></td></tr></table></figure>
<p>1）会话掌握资源，用完要回收 - 上下文管理器<br>
2）初始化会话对象时的参数<br>
graph=None 运行哪一张图<br>
target：如果将此参数留空（默认设置），<br>
会话将仅使用本地计算机中的设备。<br>
可以指定 grpc:// 网址，以便指定 TensorFlow 服务器的地址，<br>
这使得会话可以访问该服务器控制的计算机上的所有设备。<br>
config：此参数允许您指定一个 tf.ConfigProto<br>
以便控制会话的行为。例如，ConfigProto协议用于打印设备使用信息<br>
3)run(fetches,feed_dict=None)<br>
3 feed操作<br>
a = tf.placeholder(tf.float32, shape=)<br>
b = tf.placeholder(tf.float32, shape=)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br></code></pre></td></tr></table></figure>
<p>with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,<br>
log_device_placement=True)) as sess:<br>
# c_t_value = sess.run(c_t)<br>
打印设备信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2 会话的run()<br><br>- run(fetches,feed_dict=None,options = None,run_metadata= None)<br>  - 通过使用session.run()运行operation<br>  - fetches:单一的operation 或者列表 元祖（其他不属于tensorflow类型不行）<br>  - feed_dict：参数允许调用者覆盖图中张良的值 运行时赋值<br>    - 与tf.placeholder搭配使用 会检查值的形状是否与占位符兼容<br><br>&gt; 使用 tf.operation.eval()也可以运行operation 但需要在会话中进行<br><br>![snipaste20230726_221500](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230726_221500.jpg)<br><br></code></pre></td></tr></table></figure>
<p>a_t = tf.constant(1)<br>
b_t = tf.constant(2)<br>
# 不提倡直接使用这种符号运算进行计算<br>
# 更经常使用tensorflow提供的函数进行计算<br>
c_t = tf.mul(a_t, b_t)<br>
sess = tf.Session()<br>
print(sess.run©)<br>
print(c.eval(session = sess))</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 3 feed操作<br><br>- placeholder提供占位符，run时候通过feed_dict指定参数<br><br></code></pre></td></tr></table></figure>
<p>a = tf.placeholder(tf.float32, shape=)<br>
b = tf.placeholder(tf.float32, shape=)<br>
sum_ab = tf.add(a,b)<br>
with tf.Session() as sess:<br>
print(“”，sess.run(sum_ab,feed_dict = {a :3.0,b:5.0}))<br>
return None</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>**代码：**<br><br></code></pre></td></tr></table></figure>
<p>def session_demo():<br>
“”&quot;<br>
会话的演示<br>
:return:<br>
“”&quot;<br>
# TensorFlow实现加法运算<br>
a_t = tf.constant(2, name=“a_t”)<br>
b_t = tf.constant(3, name=“a_t”)<br>
c_t = tf.add(a_t, b_t, name=“c_t”)<br>
print(“a_t:\n”, a_t)<br>
print(“b_t:\n”, b_t)<br>
print(“c_t:\n”, c_t)<br>
# print(“c_t.eval():\n”, c_t.eval())</p>
<pre><code># 查看默认图
# 方法1：调用方法
default_g = tf.get_default_graph()
print(&quot;default_g:\n&quot;, default_g)

# 方法2：查看属性
print(&quot;a_t的图属性：\n&quot;, a_t.graph)
print(&quot;c_t的图属性：\n&quot;, c_t.graph)


# 开启会话
with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,
                                      log_device_placement=True)) as sess:
    # c_t_value = sess.run(c_t)
    # 试图运行自定义图中的数据、操作
    # c_new_value = sess.run((c_new))
    # print(&quot;c_new_value:\n&quot;, c_new_value)
    a, b, c = sess.run([a_t, b_t, c_t])
    print(&quot;abc:\n&quot;, a, b, c)
    print(&quot;c_t_value:\n&quot;, c_t.eval())
    print(&quot;sess的图属性：\n&quot;, sess.graph)
    print(&quot;c_t_value:\n&quot;, c_t.eval())
    print(&quot;sess的图属性：\n&quot;, sess.graph)
    # 1）将图写入本地生成events文件
    tf.summary.FileWriter(&quot;./tmp/summary&quot;, graph=sess.graph)
return None
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br><br><br>报错：<br><br>- RunTimeError:如果这Session是无效状态 已关闭<br>- TypeError:如果fetches或者feed_dict类型不合适<br>- ValueError：如果fetches或feed_dict键无效或引用Tensor不存在的键<br><br>## 2.4 张量<br><br></code></pre></td></tr></table></figure>
<p>张量 在计算机当中如何存储？<br>
标量 一个数字                 0阶张量<br>
向量 一维数组 [2, 3, 4]       1阶张量<br>
矩阵 二维数组 [[2, 3, 4],     2阶张量<br>
[2, 3, 4]]<br>
……<br>
张量 n维数组                  n阶张量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2.4.1 张量<br><br>n维数组 ndarray 类型为tf.Tensor<br><br>- type:数据类型<br><br>- shape:形状（阶）<br><br>### 1 张量的类型<br><br>![snipaste20230726_222626](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230726_222626.jpg)<br><br>### 2 张量的阶<br><br></code></pre></td></tr></table></figure>
<p>tensor1 = tf.constant(4.0)<br>
tensor2 = tf.constant([1,2,3,4])<br>
linear_squares = tf.constant([[4],[9],[16],[25]],dtype = tf.int32)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>输出：<br><br></code></pre></td></tr></table></figure>
<p>tensor1:<br>
Tensor(“Const:0”, shape=(), dtype=float32)<br>
tensor2:<br>
Tensor(“Const_1:0”, shape=(4,), dtype=int32)<br>
linear_squares_before:<br>
Tensor(“Const_2:0”, shape=(4, 1), dtype=int32)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br></code></pre></td></tr></table></figure>
<p>创建张量的时候，如果不指定类型<br>
默认 tf.float32<br>
整型 tf.int32<br>
浮点型 tf.float32</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2.4.2 创建张量的指令<br><br>- 固定值张量<br><br>  ![snipaste20230726_223130](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230726_223130.jpg)<br><br>- 随机值张量<br><br>![snipaste20230726_223248](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230726_223248.jpg)<br><br>### 2.4.3 张量的变换<br><br>#### 1 类型改变<br><br>提供了如下一些改变张量中数值类型的函数<br><br></code></pre></td></tr></table></figure>
<p>ndarray属性的修改<br>
类型的修改<br>
1）ndarray.astype(type)<br>
tf.cast(tensor, dtype)<br>
不会改变原始的tensor<br>
返回新的改变类型后的tensor<br>
2）ndarray.tostring()<br>
形状的修改<br>
1）ndarray.reshape(shape)<br>
-1 自动计算形状<br>
2）ndarray.resize(shape)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>![snipaste20230726_223648](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230726_223648.jpg)<br><br>#### 2 形状修改<br><br></code></pre></td></tr></table></figure>
<p>静态形状 - 初始创建张量时的形状<br>
1）如何改变静态形状<br>
什么情况下才可以改变/更新静态形状？<br>
只有在形状没有完全固定下来的情况下<br>
tensor.set_shape(shape)<br>
2）如何改变动态形状<br>
tf.reshape(tensor, shape)<br>
不会改变原始的tensor<br>
返回新的改变形状后的tensor<br>
动态创建新张量时，张量的元素个数必须匹配</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>- tf.reshape<br>- tf.set_shape<br><br></code></pre></td></tr></table></figure>
<p>a_p:<br>
Tensor(“Placeholder:0”, shape=(None, None), dtype=float32)<br>
b_p:<br>
Tensor(“Placeholder_1:0”, shape=(None, 10), dtype=float32)<br>
c_p:<br>
Tensor(“Placeholder_2:0”, shape=(3, 2), dtype=float32)<br>
None的部分是属于可以更新的部分 也就是静态形状可以更改的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br></code></pre></td></tr></table></figure>
<p>def tensor_demo():<br>
“”&quot;<br>
张量的演示<br>
:return:<br>
“”&quot;<br>
tensor1 = tf.constant(4.0)<br>
tensor2 = tf.constant([1, 2, 3, 4])<br>
linear_squares = tf.constant([[4], [9], [16], [25]], dtype=tf.int32)</p>
<pre><code>print(&quot;tensor1:\n&quot;, tensor1)
print(&quot;tensor2:\n&quot;, tensor2)
print(&quot;linear_squares_before:\n&quot;, linear_squares)
# 张量类型的修改
l_cast =  tf.cast(linear_squares,dtype=tf.float32)
print(&quot;linear_squares_after:\n&quot;, linear_squares)
print(&quot;l_cast:\n&quot;, l_cast)
# 更新/改变静态形状
# 定义占位符
# 没有完全固定下来的静态形状
a_p = tf.placeholder(dtype=tf.float32, shape=[None, None])
b_p = tf.placeholder(dtype=tf.float32, shape=[None, 10])
c_p = tf.placeholder(dtype=tf.float32, shape=[3, 2])
print(&quot;a_p:\n&quot;, a_p)
print(&quot;b_p:\n&quot;, b_p)
print(&quot;c_p:\n&quot;, c_p)
#更新形状未确定的部分
a_p.set_shape([2, 3])
b_p.set_shape([2, 10])
print(&quot;a_p:\n&quot;, a_p)
print(&quot;b_p:\n&quot;, b_p)
# 动态形状修改
a_p_reshape = tf.reshape(a_p, shape=[2, 3, 1])
print(&quot;a_p:\n&quot;, a_p)
# print(&quot;b_p:\n&quot;, b_p)
print(&quot;a_p_reshape:\n&quot;, a_p_reshape)
c_p_reshape = tf.reshape(c_p, shape=[2, 3])
print(&quot;c_p:\n&quot;, c_p)
print(&quot;c_p_reshape:\n&quot;, c_p_reshape)
return None
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2.4.4 张量的数学运算<br><br>- 算数运算符<br>- 基本数学函数<br>- 矩阵运算<br>- reduce操作<br>- 序列索引操作<br><br>连接：https://www.tensorflow.org/versions/r1.8/api_guides/python/math_ops<br><br>## 2.5 变量OP<br><br></code></pre></td></tr></table></figure>
<p>TensorFlow - 变量<br>
存储模型参数<br>
存储持久化<br>
可修改值<br>
指定被训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2.5.1 创建变量<br><br>- tf.Variable(initial_value = None,trainable = True,collections = None,name = None)<br><br>  - initial_value: 初始化的值<br><br>  - trainable：是否被训练<br><br>  - collections：新变量将添加到列出的图的集合中collections,默认为[GraphKeys.GLOBAL_VARIABLES],如果trainable是True变量也被添加到图形集合GraphKeys.GLOBAL_VARIABLES<br><br>- 变量需要显式初始化 才能运行值<br><br></code></pre></td></tr></table></figure>
<p>def variable_demo():<br>
“”&quot;<br>
变量的演示<br>
:return:<br>
“”&quot;<br>
#创建变量<br>
a = tf.Variable(initial_value=50)<br>
b = tf.Variable(initial_value=40)<br>
c= tf.add(a,b)<br>
print(“a:\n”, a)<br>
print(“b:\n”, b)<br>
print(“c:\n”, c)<br>
# 初始化变量 这一步非常重要<br>
init = tf.global_variables_initializer()<br>
# 开启会话<br>
with tf.Session() as sess:<br>
#这一步也很重要<br>
sess.run(init)<br>
a_value,b_value,c_value = sess.run([a,b,c])<br>
print(“a_value:\n”, a_value)<br>
print(“b_value:\n”, b_value)<br>
print(“c_value:\n”, c_value)</p>
<pre><code>  return None
</code></pre>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2.5.2 使用tf.variable.scope()修改变量的命名空间<br><br>会在OP的名字前面增加命名空间的指定名字<br><br></code></pre></td></tr></table></figure>
<p>with tf.variable_scope(“name”):<br>
var = tf.Variable(name = ‘var’,initial_value = [4],dtype = tf.float32)<br>
var_double = tf.Variable(name = ‘var’,initial_value = [4],dtype = tf.float32)<br>
&lt;tf.Variable ‘name/var:0’ shape=() dtype=float32_ref&gt;<br>
&lt;tf.Variable ‘name/var_1:0’ shape=() dtype=float32_ref&gt;</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br><br><br><br>## 2.6 高级API<br><br>### 2.6.1 其他基础API<br><br>### 1 tf.app<br><br>这个模块相当于为Tensorflow进行的脚本提供一个main函数入口 可以定义脚本运行的flags<br><br>### 2 tf.image<br><br>图像处理操作 颜色变换 变形图像编码解码<br><br>### 3 tf.gfile<br><br>提供文件操作函数<br><br>### 4 tf.summary<br><br>用来生成TensorBoard可用的统计日志 四种类型：audio image histogram scalar<br><br>### 5 tf.python_io<br><br>读写TFRecords文件<br><br>### 6 tf.train<br><br>提供一些训练器 与tf.nn结合 实现网格优化计算<br><br>### 7 tf.nn<br><br>构建神经网络的底层函数 构建网络的核心模块 添加各种层的函数比如卷积层 池化层<br><br>### 2.6.2 高级API<br><br>### 1 tf.keras<br><br>独立的深度学习库 快速构建模型<br><br>### 2 tf.layers<br><br>以更高级的概念层定义一个模型<br><br>### 3 tf.contrib<br><br>提供将计算图中的网络层 正则化 摘要操作 是构建计算图的高级操作 但tf.contrib包含不稳定和实验代码<br><br>### 4 tf.estimator<br><br>一个Estimator相当于model+train+evaluate<br><br>实现了简单地分类器和回归器 dnn baseline learing dnn只是全连接网络<br><br>## 2.7 Tensorflow实现线性回归<br><br>### 2.7.1 线性回归原理复习<br><br></code></pre></td></tr></table></figure>
<p>1）构建模型<br>
y = w1x1 + w2x2 + …… + wnxn + b<br>
2）构造损失函数<br>
均方误差<br>
3）优化损失<br>
梯度下降</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2.7.2 案例：实现线性回归的训练<br><br>### 1 案例确定<br><br>- 假设随机指定100个点 只有一个特征<br>- 数据本身分布为y = 0.8 *x +0.7<br><br></code></pre></td></tr></table></figure>
<p>准备真实数据<br>
100样本<br>
x 特征值 形状 (100, 1)<br>
y_true 目标值 (100, 1)<br>
y_true = 0.8x + 0.7<br>
假定x 和 y 之间的关系 满足<br>
y = kx + b<br>
k ≈ 0.8 b ≈ 0.7<br>
流程分析：<br>
(100, 1) * (1, 1) = (100, 1)<br>
y_predict = x * weights(1, 1) + bias(1, 1)<br>
1）构建模型<br>
y_predict = tf.matmul(x, weights) + bias<br>
2）构造损失函数<br>
error = tf.reduce_mean(tf.square(y_predict - y_true))<br>
3）优化损失<br>
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)<br>
5 学习率的设置、步数的设置与梯度爆炸</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2 API<br><br>运算<br><br>- 矩阵运算<br>  - tf.matmul(x,w)<br><br>- 平方<br>  - tf.square(error)<br><br>- 均值<br>  - tf.reduce_mean(error)<br><br>**梯度下降优化**<br><br>- tf.train.GradientDescentOptimizer(learing_rate)<br><br>  - 梯度下降优化<br><br>  - learing_rate:学习率 一般为0~1之间较小的值<br><br>  - method:<br><br>    - minimize(loss)<br>    <br>  - return: 梯度下降OP<br><br>### 3 步骤分析<br><br>### 学习率的设置 步数的设置以及梯度爆炸<br><br>学习率越大 训练到较好的结果步数越小 学习率越小 训练到较好的结果步数越大<br><br>但是学习过大会出现梯度爆炸现象 <br><br></code></pre></td></tr></table></figure>
<p>如何解决梯度爆炸<br>
重新设计网络<br>
调整学习率<br>
使用梯度截断（在训练过程中检查和限制梯度大小）<br>
使用激活函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 4 变量的trainable设置观察<br><br>trainable的参数作用 指定是否训练<br><br>`weight = tf.Variable(tf.random([1,1],mean = 0.0,stddev = 1.0),na)`<br><br>### 2.7.3 增加其他功能<br><br>- 变量Tensorboard显示<br>- 增加命名空间<br>- 模型保存与加载<br>- 命令行参数设置<br><br>### 1 增加变量显示<br><br>**目的：在Tensorboard中观察模型的参数 损失值等变量值的变化**<br><br>- 1、收集变量<br>  - tf.summary.scalar(name = &quot;&quot;,tensor)收集对于损失函数和准确率等单值变量,name为变量的名字，tensor为值 标量<br>  - tf.summary.histogram(name = &quot;&quot;,tensor)收集高纬度的变量参数 直方图分布状况<br>  - tf.summary.image(name = &quot;&quot;,tensor)收集输入的图片张量能显示图片 <br>- 2、合并变量写入事件文件中<br>  - merged = tf.summary.merge_all()<br>  - 运行合并：summary = sess.run(merged) 每次迭代都需运行<br>  - 添加：FileWriter.add_summary(summary,i),i表示第几次的值<br><br></code></pre></td></tr></table></figure>
<p>1）创建事件文件<br>
2）收集变量<br>
3）合并变量<br>
4）每次迭代运行一次合并变量<br>
5）每次迭代将summary对象写入事件文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 2 增加命名空间<br><br>使代码结构更清晰 Tensorboard图结构清楚<br><br>`with tf.variable_scope(&quot;lr_model&quot;)`<br><br>### 3 模型的保存与加载<br><br>- tf.train.Saver(var_list = None,max_to_keep = 5)<br><br>  - 保存和加载模型（保存文件格式：checkpoint文件）<br><br>  - var_list:指定将要保存和还原的变量 可以作为一个dict 或者一个列表传递<br><br>  - max_to_keep:指示要保留的最近检查点文件的最大数量 创建新文件时 会删除比较旧的文件 如果无或者0 就保留所有检查点的文件<br>  <br><br></code></pre></td></tr></table></figure>
<p>saver = tf.train.Saver(var_list=None,max_to_keep=5)<br>
1）实例化Saver<br>
2）保存<br>
saver.save(sess, path)<br>
3）加载<br>
saver.restore(sess, path)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>使用<br><br></code></pre></td></tr></table></figure>
<p>指定目录+模型名字<br>
saver.save(sess,‘./tmp/ckpt/test/myregression.ckpt’)<br>
saver.restore(sess,‘./tmp/ckpt/test/myregression.ckpt’)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>如果要判断模型是否存在 直接指定目录<br><br></code></pre></td></tr></table></figure>
<p>checkpoint = tf.train.latest_checkpoint(“./tmp/model/”)<br>
saver.restore(sess.checkpoint)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 4 命令行参数使用<br><br>![snipaste20230728_182253](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230728_182253.jpg)<br><br></code></pre></td></tr></table></figure>
<p>1）tf.app.flags<br>
tf.app.flags.DEFINE_integer(“max_step”, 0, “训练模型的步数”)<br>
tf.app.flags.DEFINE_string(“model_dir”, &quot; &quot;, “模型保存的路径+模型名字”)<br>
2）FLAGS = tf.app.flags.FLAGS<br>
通过FLAGS.max_step调用命令行中传过来的参数<br>
3、通过tf.app.run()启动main(argv)函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br></code></pre></td></tr></table></figure>
<p>#定义一些常用的命令行参数<br>
#训练步数<br>
tf.app.flags.DEFINE_integer(“max_step”,0,“训练模型的步数”)<br>
#定义模型的路径<br>
tf.app.flags.DEFINE_string(“model_dir”,&quot; &quot;,“模型的保存路径+模型名字”)</p>
<p>#定义获取命令行参数<br>
FLAGS = tf.app.flags.FLAG<br>
#开启训练<br>
#训练的步数（依据模型的大小而定）<br>
for i in range(FLAGS.max_step):<br>
sess.run(train_op)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 完整代码：<br><br>```python<br>def linear_regression():<br>    &quot;&quot;&quot;<br>    自实现一个线性回归<br>    :return:<br>    &quot;&quot;&quot;<br>    with tf.variable_scope(&quot;prepare_data&quot;):<br>        # 1）准备数据<br>        X = tf.random_normal(shape=[100, 1], name=&quot;feature&quot;)<br>        y_true = tf.matmul(X, [[0.8]]) + 0.7<br><br>    with tf.variable_scope(&quot;create_model&quot;):<br>        # 2）构造模型<br>        # 定义模型参数 用 变量<br>        weights = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name=&quot;Weights&quot;)<br>        bias = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name=&quot;Bias&quot;)<br>        y_predict = tf.matmul(X, weights) + bias<br><br>    with tf.variable_scope(&quot;loss_function&quot;):<br>        # 3）构造损失函数<br>        error = tf.reduce_mean(tf.square(y_predict - y_true))<br><br>    with tf.variable_scope(&quot;optimizer&quot;):<br>        # 4）优化损失<br>        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)<br><br>    # 2_收集变量<br>    tf.summary.scalar(&quot;error&quot;, error)<br>    tf.summary.histogram(&quot;weights&quot;, weights)<br>    tf.summary.histogram(&quot;bias&quot;, bias)<br><br>    # 3_合并变量<br>    merged = tf.summary.merge_all()<br><br>    # 创建Saver对象<br>    saver = tf.train.Saver()<br><br>    # 显式地初始化变量<br>    init = tf.global_variables_initializer()<br><br>    # 开启会话<br>    with tf.Session() as sess:<br>        # 初始化变量<br>        sess.run(init)<br><br>        # 1_创建事件文件<br>        file_writer = tf.summary.FileWriter(&quot;./tmp/linear&quot;, graph=sess.graph)<br><br>        # 查看初始化模型参数之后的值<br>        print(&quot;训练前模型参数为：权重%f，偏置%f，损失为%f&quot; % (weights.eval(), bias.eval(), error.eval()))<br><br>        # 开始训练<br>        # for i in range(100):<br>        #     sess.run(optimizer)<br>        #     print(&quot;第%d次训练后模型参数为：权重%f，偏置%f，损失为%f&quot; % (i+1, weights.eval(), bias.eval(), error.eval()))<br>        #<br>        #     # 运行合并变量操作<br>        #     summary = sess.run(merged)<br>        #     # 将每次迭代后的变量写入事件文件<br>        #     file_writer.add_summary(summary, i)<br>        #<br>        #     # 保存模型<br>        #     if i % 10 ==0:<br>        #         saver.save(sess, &quot;./tmp/model/my_linear.ckpt&quot;)<br>        # 加载模型<br>        if os.path.exists(&quot;./tmp/model/checkpoint&quot;):<br>            saver.restore(sess, &quot;./tmp/model/my_linear.ckpt&quot;)<br><br>        print(&quot;训练后模型参数为：权重%f，偏置%f，损失为%f&quot; % (weights.eval(), bias.eval(), error.eval()))<br><br><br>    return None<br><br># 1）定义命令行参数<br>tf.app.flags.DEFINE_integer(&quot;max_step&quot;, 100, &quot;训练模型的步数&quot;)<br>tf.app.flags.DEFINE_string(&quot;model_dir&quot;, &quot;Unknown&quot;, &quot;模型保存的路径+模型名字&quot;)<br><br># 2）简化变量名<br>FLAGS = tf.app.flags.FLAGS<br><br>def command_demo():<br>    &quot;&quot;&quot;<br>    命令行参数演示<br>    :return:<br>    &quot;&quot;&quot;<br>    print(&quot;max_step:\n&quot;, FLAGS.max_step)<br>    print(&quot;model_dir:\n&quot;, FLAGS.model_dir)<br><br>    return None<br><br>def main(argv):<br>    print(&quot;code start&quot;)<br>    return None<br><br></code></pre></td></tr></table></figure>
<h2 id="3-1-文件读取流程">3.1 文件读取流程</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">数据IO操作<br>    三种<br>     占位符 &amp; feed_dict搭配使用<br>     QueueRunner：基于队列的输入管道从Tensorflow图形开头的文件中读取数据<br>     Feeding:运行每一步时 python提供数据<br>     预加载数据：图中张量包含所有数据（小数据集）<br>        通用文件读取流程<br>            图片<br>            二进制数据<br>            TFRecords<br>神经网络基础<br>    神经网络原理<br>    手写数字识别案例<br></code></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">多线程 + 队列<br></code></pre></td></tr></table></figure>
<h3 id="3-1-1-文件读取流程">3.1.1 文件读取流程</h3>
<ul>
<li>第一阶段： 构造文件名队列</li>
<li>第二阶段：读取与解码</li>
<li>第三阶段：批处理</li>
</ul>
<p>注：这些操作需要启动运行这些队列操作的线程 以便我们在进行文件读取的过程中能够顺利进行入队出队操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1）构造文件名队列<br>    file_queue = tf.train.string_input_producer(string_tensor,shuffle=True)<br>2）读取与解码<br>    文本：<br>        读取：tf.TextLineReader()<br>        解码：tf.decode_csv()<br>    图片：<br>        读取：tf.WholeFileReader()<br>        解码：<br>            tf.image.decode_jpeg(contents)<br>            tf.image.decode_png(contents)<br>    二进制：<br>        读取：tf.FixedLengthRecordReader(record_bytes)<br>        解码：tf.decode_raw()<br>    TFRecords<br>        读取：tf.TFRecordReader()<br>    key, value = 读取器.read(file_queue)<br>    key：文件名<br>    value：一个样本<br>3）批处理队列<br>    tf.train.batch(tensors, batch_size, num_threads = 1, capacity = 32, name=None)<br>手动开启线程<br>    tf.train.QueueRunner()<br>    开启会话：<br>        tf.train.start_queue_runners(sess=None, coord=None)<br></code></pre></td></tr></table></figure>
<h3 id="1-构造文件名队列">1 构造文件名队列</h3>
<p>将需要读取的文件的文件名放入文件名队列</p>
<ul>
<li>tf.train.string_input_producer(string_tensor,shuffle = True) 后面这个是顺序
<ul>
<li>string_tensor:含有文件名+路径的1阶张量 一般用列表</li>
<li>num_epochs:过几遍数据 默认无限过数据</li>
<li>return 文件队列</li>
</ul>
</li>
</ul>
<h3 id="2-读取与解码">2 读取与解码</h3>
<p>从队列当中读取文件内容 并进行解码操作</p>
<h4 id="1）-读取文件内容">1） 读取文件内容</h4>
<p><strong>阅读器默认每次只读取一个样本</strong></p>
<p>具体来说：</p>
<p>文本文件默认一次读取一行 图片文件默认一次读取一张照片 二进制文件默认一次读取指定字节数 TFRecords默认一次读取一个example</p>
<ul>
<li>
<p><strong>tf.TextLineReader</strong>()</p>
<ul>
<li>阅读文本文件逗号分隔值（CSV)格式 默认按行读取</li>
<li>return:读取器实例</li>
</ul>
</li>
<li>
<p><strong>tf.WholeFileReader</strong>()图片</p>
<ul>
<li>return:读取器实例</li>
</ul>
</li>
<li>
<p><strong>tf.FixedLengthRecordReader</strong>(record_bytes)</p>
<ul>
<li>record_bytes: 整型 每次读取一个样本的字节数</li>
</ul>
</li>
<li>
<p><strong>tf.TFRecordReader()</strong></p>
<ul>
<li>return:读取器实例</li>
</ul>
</li>
</ul>
<blockquote>
<p>1 共同读取方法：read(file_queue)并返回一个Tensors元祖（key:文件名 value:一个样本）</p>
<p>2 由于默认只会读取一个样本 所以如果想要进行批处理 需要使用tf.train.batch或者tf.train.shuffle_batch进行批处理 便于之后指定每批次多个样本的训练</p>
</blockquote>
<h4 id="2）-内容解码">2） 内容解码</h4>
<p>读取不同类型的文件 也应该对不同类型文件解码成统一的Tensor格式</p>
<ul>
<li><strong>tf.decode_csv()</strong></li>
<li><strong>tf.image.decode_jpeg(contents)</strong>
<ul>
<li>解码为unit8张量</li>
<li>return unit8张量 3-D形状[height,width,channels]</li>
</ul>
</li>
<li><strong>tf.image.decode_png(contents)</strong>
<ul>
<li>解码为unit8张量</li>
<li>return 张量类型 3-D形状[height,width,channels]</li>
</ul>
</li>
<li><strong>tf.image.decode_raw</strong></li>
</ul>
<p>后面可以用tf.cast转换</p>
<h3 id="3-批处理">3 批处理</h3>
<p>解码之后 可以直接获取一个样本内容 如果想获取多个样本 需要加入到新的队列进行批处理</p>
<ul>
<li>
<p>tf.train_batch(tensors,batch_size,num_threads = 1,capacity = 32,name = None)</p>
<ul>
<li>读取指定大小（个数）的张量</li>
<li>tensors:可以使包含张量的列表</li>
<li>batch_size:从队列中读取的批处理大小</li>
<li>num_threads:进入队列的线程数</li>
<li>capacity:整数 队列中元素的最大数量</li>
<li>return :tensors</li>
</ul>
</li>
<li>
<p>tf.train.shuffle_batch</p>
</li>
</ul>
<h3 id="3-1-2-线程操作">3.1.2 线程操作</h3>
<p>以上用到的队列都是QueueRunner对象</p>
<p>每个QueueRunner都负责一个阶段 tf.train.QueueRunner()函数会要求图中每个QueueRunner启动它的运行队列操作的线程</p>
<ul>
<li>
<p><strong>tf.train.start_queue_runners(sess=None, coord=None)</strong></p>
<ul>
<li>收集图中所有的队列线程 默认同时启动线程</li>
<li>sess:所在的会话</li>
<li>coord:现成协调器</li>
<li>return 返回所有线程</li>
</ul>
</li>
<li>
<p><strong>tf.train.Coordinate()</strong></p>
<ul>
<li>线程协调员 对线程进行管理和协调</li>
<li>request_stop():请求停止</li>
<li>should_stop():询问是否结束</li>
<li>join(threads = None,stop_grace_period_secs=120):回收线程</li>
<li>return:线程协调员实例</li>
</ul>
</li>
</ul>
<h2 id="3-2-图片数据">3.2 图片数据</h2>
<h3 id="3-2-1-图像基本知识">3.2.1 图像基本知识</h3>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">文本  特征词 -&gt; 二维数组 shape n_samples m_features<br>字典  one-hot -&gt; 二维数组 shape n_samples m_features<br>图片  最基本单位：像素值 shape n_samples m_features<br>1 图片三要素<br>    黑白图、灰度图<br>        一个通道<br>            黑[0, 255]白<br>    彩色图<br>        三个通道<br>            一个像素点 三个通道值构成<br>            R [0, 255]<br>            G [0, 255]<br>            B [0, 255]<br>2 TensorFlow中表示图片<br>    Tensor对象<br>        指令名称、形状、类型<br>        shape = [height, width, channel]<br>3 图片特征值处理<br>    [samples, features]<br>    为什么要缩放图片到统一大小？<br>    1）每一个样本特征数量要一样多<br>    2）缩小图片的大小<br>    tf.image.resize_images(images, size)<br>4 数据格式<br>    存储：uint8<br>    训练：float32<br></code></pre></td></tr></table></figure>
<h3 id="1-图片三要素">1 图片三要素</h3>
<p>图片长度 图片宽度 图片通道数</p>
<p>像素数量= 长 * 宽 * 通道数目</p>
<h3 id="2-张量形状">2 张量形状</h3>
<ul>
<li>单张图片 [height, width, channel]</li>
<li>多个图片 [batch, height,width,channel] batch表示一个批次的张量数目</li>
</ul>
<h3 id="3-2-2-图片特征值处理">3.2.2 图片特征值处理</h3>
<p>在进行图像识别的时候 每个图片样本特征数量要保持相同 所以需要将所有图片张量大小进行统一转换</p>
<p>另一个方面 如果图片的像素量太大 通过这种方式适当减少像素的数量 减少训练计算开销</p>
<ul>
<li>
<p>tf.image.resize_images(images,size)</p>
<ul>
<li>
<p>缩小放大图片</p>
</li>
<li>
<p>images:4-D形状 就上面说的单张图片和多张图片的张量形状</p>
<ul>
<li>
<p>单张图片 [height, width, channel]</p>
</li>
<li>
<p>多个图片 [batch, height,width,channel] batch表示一个批次的张量数目</p>
</li>
</ul>
</li>
<li>
<p>images:4-D形状 就上面说的单张图片和多张图片的张量形状</p>
</li>
<li>
<p>size:1-D int32张量 new_height new_width 图像新尺寸</p>
</li>
<li>
<p>return 4-D 或者3-D格式图片</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-2-3-数据格式">3.2.3 数据格式</h3>
<ul>
<li>存储：unit8节约空间</li>
<li>矩阵计算 float32 提高精度</li>
</ul>
<h3 id="3-2-4-案例：狗图片读取案例">3.2.4 案例：狗图片读取案例</h3>
<h4 id="1-读取流程分析">1 读取流程分析</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">1）构造文件名队列<br>    file_queue = tf.train.string_input_producer(string_tensor,shuffle=True)<br>2）读取与解码<br>    形状和类型保持统一<br>    读取：<br>        reader = tf.WholeFileReader()<br>        key, value = reader.read(file_queue)<br>    解码：<br>        image_decoded = tf.image.decode_jpeg(value)<br>3）批处理队列<br>    image_decoded = tf.train.batch([image_decoded], 100, num_threads = 2, capacity=100)<br>手动开启线程<br></code></pre></td></tr></table></figure>
<h4 id="2-代码">2 代码</h4>
<ul>
<li>
<pre><code>import tensorflow._api.v2.compat.v1 as tf
tf.compat.v1.disable_eager_execution()
import os
def picture_demo(file_list):
    &quot;&quot;&quot;
    狗图片读取案例
    :return:
    &quot;&quot;&quot;
    # 1构造文件名队列
    file_queue = tf.train.string_input_producer(file_list)
    # 2读取与解码
    # 读取阶段
    reader = tf.WholeFileReader()
    #key 文件名 value 一张图片的原始编码形式
    key,value = reader.read(file_queue)
    print(&quot;key:\n&quot;,key)
    print(&quot;value:\n&quot;,value)
    # 解码阶段
    image = tf.image.decode_image(value)
    # 图像形状 类型修改
    image_resized = tf.image.resize_images(image, [200, 200])
    # 静态形状修改
    image_resized.set_shape(shape=[200,200,3])
    # 3批处理
    image_batch = tf.train.batch([image_resized],batch_size=100,num_threads=1,capacity=100)
    
    
    # 开启会话
    with tf.Session() as sess:
        # 开启线程
        # 线程协调员
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess,coord=coord)
        key_new,value_new = sess.run([key,value])
        # print(&quot;key_new:\n&quot;,key_new)
        # print(&quot;value_new\n&quot;,value_new)
    #     回收线程
        coord.request_stop()
        coord.join(threads)
    return None
if __name__ == &quot;__main__&quot;:
  #   构建文件名的列表
  file_name = os.listdir(&quot;./dog&quot;)
  # print(file_name)
  # 拼接路径和文件名
  file_list = [os.path.join(&quot;./dog/&quot;,file) for file in file_name]
  # print(file_list)
  picture_demo(file_list)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>## 3.3. 二进制数据<br><br>### 3.3.1 CIFAR10二进制数据集介绍<br><br></code></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>- 二进制版本数据文件<br><br>每3073个字节为一个样本<br><br>1个目标值+3072像素（1024字节红色通道+1024字节绿色通道+1024字节蓝色通道）<br><br>### 3.3.2 CIFAR10二进制数据读取<br><br>#### 1 分析<br><br></code></pre></td></tr></table></figure>
<p>1）构造文件名队列<br>
2）读取与解码<br>
reader = tf.FixedLengthRecordReader(3073)<br>
key,value= reader.read(file_queue)<br>
decoded = tf.decode_raw(value,tf.unit8)<br>
对tensor对象进行切片<br>
目标值label 特征值image(3072字节)<br>
[r[32,32]],<br>
[g[32,32]].<br>
[b[32,32]]<br>
shape = (3,32,32) = (channels,height,width)<br>
–&gt;tensorflow图像表示习惯<br>
图片的形状 类型调整完毕<br>
3）批处理队列<br>
开启会话<br>
手动开启线程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>#### 2 代码<br><br>- 定义CIFAR类 设置图片相关属性<br><br></code></pre></td></tr></table></figure>
<p>import tensorflow._api.v2.compat.v1 as tf<br>
tf.compat.v1.disable_eager_execution()<br>
import os<br>
class Cifar(object):<br>
def <strong>init</strong>(self):<br>
# 初始化操作<br>
self.height = 32<br>
self.width = 32<br>
self.channels = 3<br>
# 字节数<br>
self.image_bytes = self.height * self.width * self.channels<br>
self.label_bytes = 1<br>
self.all_bytes = self.label_bytes+self.image_bytes<br>
def read_and_decode(self,file_list):<br>
# 1、构建文件名队列<br>
file_queue = tf.train.string_input_producer(file_list)<br>
# 2、读取的解码<br>
reader = tf.FixedLengthRecordReader(self.all_bytes)<br>
# key 文件名 value 一个样本<br>
key, value = reader.read(file_queue)<br>
print(“key:\n”, key)<br>
print(“value:\n”, value)<br>
# 解码阶段<br>
decoded = tf.decode_raw(value,tf.uint8)<br>
print(“decoded:\n”,decoded)<br>
# 将目标值和特征值切片切开<br>
label = tf.slice(decoded,[0],[self.label_bytes])<br>
image = tf.slice(decoded,[self.label_bytes],[self.image_bytes])<br>
print(“lable:\n”,label)<br>
print(“image:\n”,image)<br>
# 调整图像形状 类型修改<br>
image_reshaped = tf.reshape(image,shape=[self.channels,self.height,self.width])<br>
print(“image_reshaped:\n”,image_reshaped)<br>
# 转置 将图片的顺序变成height width channels<br>
image_transposed = tf.transpose(image_reshaped,[1,2,0])<br>
print(“image_transposed:\n”,image_transposed)<br>
# 调整图像类型<br>
image_cast = tf.cast(image_transposed,tf.float32)<br>
# 3、批处理<br>
label_batch, image_batch = tf.train.batch([label,image_cast], batch_size=100, num_threads=1, capacity=100)<br>
print(“lable_batch:\n”,label_batch)<br>
print(“image_batch:\n”,image_batch)<br>
# 开启会话<br>
with tf.Session() as sess:<br>
# 开启线程<br>
# 线程协调员<br>
coord = tf.train.Coordinator()<br>
threads = tf.train.start_queue_runners(sess=sess, coord=coord)<br>
key_new, value_new, decoded_new, label_new, image_new,image_reshaped_new, image_transposed_new = sess.run([key, value,decoded,label,image,image_reshaped,image_transposed])<br>
print(“key_new:\n”, key_new)<br>
print(“value_new\n”,value_new)<br>
print(“decoded_new\n”, decoded_new)<br>
print(“label_new\n”, label_new)<br>
print(“image_new\n”, image_new)<br>
print(“image_reshaped_new:\n”, image_reshaped_new)<br>
print(“image_transposed_new:\n”,image_transposed_new)<br>
# 回收线程<br>
coord.request_stop()<br>
coord.join(threads)<br>
return None<br>
if <strong>name</strong> == “<strong>main</strong>”:<br>
#   构建文件名的列表<br>
file_name = os.listdir(“./cifar-10-batches-bin”)<br>
print(“file_name:\n”,file_name)<br>
file_list = [os.path.join(“./cifar-10-batches-bin/”,file) for file in file_name if file[-3:] == “bin”]<br>
print(“file_list:\n”, file_list)<br>
# 实例化Cifar<br>
cifar = Cifar()<br>
cifar.read_and_decode(file_list)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>## 3.4 TFRecords<br><br>### 3.4.1 什么是TFRecords文件<br><br>二进制文件 不需要单独的标签文件<br><br>使用步骤：1）获取数据<br><br>​                    2）将数据填入到Example协议内存块（protocol buffer）<br><br>         3） 将协议内存块序列化为字符串 并通过tf.python_io.TFRecordWriter写入到TFRecords文件<br><br><br>​      <br><br>*.tfrecords<br><br>### 3.4.2 Example结构解析<br><br></code></pre></td></tr></table></figure>
<p>cifar10<br>
特征值 - image - 3072个字节<br>
目标值 - label - 1个字节<br>
example = tf.train.Example(features=tf.train.Features(feature={<br>
“image”:tf.train.Feature(bytes_list=tf.train. BytesList(value=[image])<br>
“label”:tf.train.Feature(int64_list=tf.train. Int64List(value=[label]))<br>
}))<br>
example.SerializeToString()</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>![](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230802_173557.jpg)<br><br>![snipaste20230802_173620](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230802_173620.jpg)<br><br>### 3.4.3 案例：CIFAR10数据存入TFRecords文件<br><br>#### 1 分析<br><br>- 构造存储实例，tf.python_io.TFRecordWriter(path)<br>  - 写入tfrecords文件<br>  - path:TFRecords文件的路径<br>  - return：写文件<br>    - method方法<br>      - write(record):向文件中写入一个example<br>      - close():关闭文件写入器<br><br>- 循环将数据填入到example协议内存块（protocol buffer）<br><br>#### 2 代码<br><br></code></pre></td></tr></table></figure>
<p>return image_value,label_value<br>
上面的代码案例返回值更改<br>
def write_to_tfrecords(self,image_batch,label_batch):<br>
“”&quot;<br>
将样本的特征值和目标值一起写入tfrecords文件<br>
:param image:<br>
:param label:<br>
:return:<br>
“”&quot;<br>
with tf.python_io.TFRecordWriter(“cifar-10-batches-bin”) as writer:<br>
# 循环构造example对象 并序列化写入文件<br>
for i in range(100):<br>
image = image_batch[i].toString()<br>
label = label_batch[i].toString()<br>
print(“tfrecords_image:\n”, image)<br>
print(“tfrecords_label:\n”, label)<br>
example = tf.train.Example(features=tf.train.Features(feature={<br>
“image”: tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),<br>
“label”: tf.train.Feature(int64_list=tf.train.Int64List(value=[label])),<br>
}))<br>
# example.SerializeToString()<br>
# 将序列化后的example写入文件<br>
writer.write(example.SerializeToString())</p>
<pre><code>    return None
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>### 3.4.4 读取TFRecords文件API<br><br>读取这种文件整个过程与其他文件一样 只不过需要有个解析Example步骤 可以使用tf.TFRecordWriter的tf.parse_single_example解析器 将协议内存块解析为张量<br><br></code></pre></td></tr></table></figure>
<p>1）构造文件名队列<br>
2）读取和解码<br>
读取<br>
解析example<br>
feature = tf.parse_single_example(value, features={<br>
“image”:tf.FixedLenFeature([], tf.string),<br>
“label”:tf.FixedLenFeature([], tf.int64)<br>
})<br>
image = feature[“image”]<br>
label = feature[“label”]<br>
解码<br>
tf.decode_raw()<br>
3）构造批处理队列</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>![](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230802_175756.jpg)<br><br>### 3.4.5 案例：读取CIFAR的TFRecords文件<br><br>#### 1 分析<br><br>- 使用tf.train.string_input_producer构造文件队列<br>- tf.TFRecordReader读取数据并解析<br>  - tf.parse_single_example 进行解析<br>- tf.decode_raw 解码<br>  - 类型是bytes需要解码<br>  - 其他不需要<br>- 处理图片数据形状以及数据类型，加入批处理队列<br>- 开启会话线程进行<br><br>#### 2 代码<br><br></code></pre></td></tr></table></figure>
<p>def read_tfrecords(self):<br>
“”&quot;<br>
读取TFRecords文件<br>
:return:<br>
“”&quot;<br>
# 1、构造文件名队列<br>
file_queue = tf.train.string_input_producer([“cifar10.tfrecords”])</p>
<pre><code>    # 2、读取与解码
    # 读取
    reader = tf.TFRecordReader()
    key, value = reader.read(file_queue)

    # 解析example
    feature = tf.parse_single_example(value, features=&#123;
        &quot;image&quot;: tf.FixedLenFeature([], tf.string),
        &quot;label&quot;: tf.FixedLenFeature([], tf.int64)
    &#125;)
    image = feature[&quot;image&quot;]
    label = feature[&quot;label&quot;]
    print(&quot;read_tf_image:\n&quot;, image)
    print(&quot;read_tf_label:\n&quot;, label)

    # 解码
    image_decoded = tf.decode_raw(image, tf.uint8)
    print(&quot;image_decoded:\n&quot;, image_decoded)
    # 图像形状调整
    image_reshaped = tf.reshape(image_decoded, [self.height, self.width, self.channel])
    print(&quot;image_reshaped:\n&quot;, image_reshaped)

    # 3、构造批处理队列
    image_batch, label_batch = tf.train.batch([image_reshaped, label], batch_size=100, num_threads=2, capacity=100)
    print(&quot;image_batch:\n&quot;, image_batch)
    print(&quot;label_batch:\n&quot;, label_batch)

    # 开启会话
    with tf.Session() as sess:

        # 开启线程
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)

        image_value, label_value = sess.run([image_batch, label_batch])
        print(&quot;image_value:\n&quot;, image_value)
        print(&quot;label_value:\n&quot;, label_value)

        # 回收资源
        coord.request_stop()
        coord.join(threads)

    return None
</code></pre>
<p>if <strong>name</strong> == “<strong>main</strong>”:<br>
cifar = Cifar()<br>
# image_value, label_value = cifar.read_binary()<br>
# cifar.write_to_tfrecords(image_value, label_value)<br>
cifar.read_tfrecords()</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>## 3.5 神经网络基础<br><br>### 3.5.1 神经网络<br><br>人工神经网络（Artificial Neural Network，简写为ANN)也简称为神经网络(NN)。**是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）结构和功能的计算模型。经典的神经网络结构包含三个层次的神经网络。**分别为输入层，输出层以及隐藏层。<br><br>![snipaste20230809_195045](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_195045.jpg)<br><br>**其中每层的圆圈代表一个神经元，隐藏层和输出层的神经元有输入的数据计算后输出，输入层的神经元只是输入**<br><br>- 神经网络的特点：<br>  - 每个连接都有个权值<br><br>  - 同一层神经元之间没有连接<br><br>  - 最后的输出结构对应的层也称之为全连接层<br>  <br><br>​     那么为什么设计这样的结构呢?首先从一个最基础的结构说起，神经元。以前也称之为感知机。神经元就是要模拟人的神经元结构。<br><br>![](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_195418.jpg)<br><br>&gt; 一个神经元通常具有多个**树突**，主要用来接受传入信息;而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做**&quot;突触”**。<br><br>**感知机（PLA : Perceptron Learning Algorithm）**<br><br>感知机就是模拟这样的大脑神经网络处理数据的过程。感知机模型如下图：<br><br>![snipaste20230809_195501](D:\_xpl2022\Desktop\markdown\picture\snipaste20230809_195501.jpg)<br><br>感知机是一种最基础的分类模型，类似于逻辑回归，不同的是，**感知机的激活函数用的是sign，而逻辑回归用的sigmoid。**感知机也具有连接的权重和偏置<br><br>![snipaste20230809_195549](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_195549.jpg)<br><br>    输入层<br>            特征值和权重 线性加权<br>            y = w1x1 + w2x2 + …… + wnxn + b<br>            细胞核-激活函数<br>                sigmoid<br>                sign<br>        隐藏层<br>        输出层<br>    单个神经元 - 感知机<br>    感知机(PLA: Perceptron Learning Algorithm))<br>        逻辑回归 <br>        线性加权 sigmoid 映射到0-1之间<br>        x1, x2<br>        w1x1 + w2x2 + b = 常数<br>        w2x2 = -w1x1 - b + 常数<br>        x2 = kx1 + b<br>        x2 = kx1 + b<br>        x1 x2<br>        单个神经元不能解决一些复杂问题<br>        1）多层神经元<br>        2）增加激活函数<br><br>## 3.6 神经网络原理<br><br>![snipaste20230809_200008](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_200008.jpg)<br><br>**神经网络解决多分类问题最常用的方法是设置n个输出节点，其中n为类别的个数。**<br><br>任意事件发生的概率都在0和1之间，且总有某一个事件发生（概率的和为1)。如果将分类问题中“一个样例属于某一个类别”看成一个概率事件，那么训练数据的正确答案就符合一个概率分布。如何将神经网络前向传播得到的结果也变成概率分布呢? Softmax回归就是一个非常常用的方法。<br><br>### 3.6.1 softmax回归<br><br>**softmax回归将神经网络输出转换成概率结果**<br><br>将原本的sigmod 变成softmax<br><br>&lt;img src=&quot;https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_200144.jpg&quot; alt=&quot;snipaste20230809_200144&quot; style=&quot;zoom:50%;&quot; /&gt;<br><br>- 如何理解<br><br></code></pre></td></tr></table></figure>
<p>假设输出结果为:2.3，4.1,5.6<br>
softmax的计算输出结果为:<br>
y1_p = e^2.3/(e^2.3+e^4.1+e^5.6)<br>
y1_p = e^4.1/(e^2.3+e^4.1+e^5.6)<br>
y1_p = e^5.6/( e^2.3+e^4.1+e^5.6)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">![snipaste20230809_200432](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_200432.jpg)<br><br>### 3.6.2交叉熵损失<br><br>&gt; **交叉熵损失函数**： 衡量神经网络预测的概率和真实结果的概率之间的距离<br>&gt;<br>&gt; 交叉熵适合用于一个样本对应一个目标值<br><br>### 1 公式<br><br>![snipaste20230809_200523](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_200523.jpg)<br><br>为了能够衡量距离，目标值需要进行one-hot编码，能与概率值――对应，如下图<br><br>![snipaste20230809_200535](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_200535.jpg)<br><br></code></pre></td></tr></table></figure>
<p>计算</p>
<p>-[0log(0.10)+0log(0.05)+0log(0.15)+0log(0.10)+0log(0.05)+0log(0.20)+1log(0.10)…]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"><br>上述的结果为-1log(0.10)，那么为了减少这一个样本的损失。神经网络应该怎么做?<br><br>答：提高对应目标值为1的位置输出概率大小。<br><br>### 2 损失大小<br><br>神经网络最后的损失为平均每个样本的损失大小<br><br>- 对所有样本的损失求和取平均值<br><br>**即对所有样本的损失求和取其平均值**<br><br>### 3.6.4 **softmax、交叉熵损失API**<br><br>- tf.nn.softmax_cross_entropy_with_logits(labels=None, logits=None, name=None)<br>  - abels：标签值（真实值）<br>  - logits：样本加权之后的值<br>  - return：返回损失值列表<br>  - 损失大小求平均<br><br>- tf.reduce_mean(input_tensor)<br>  - 计算张量的尺寸的元素平均值<br>  - 交叉熵的优化：梯度下降<br><br>## 3.7 案例：Mnist手写数字识别<br><br>### 3.7.1 数据集介绍<br><br>![snipaste20230809_201054](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_201054.jpg)<br><br>文件说明:<br><br>- train-images-idx3-ubyte.gz: training set images(9912422 bytes)<br><br>- train-labels-idx1-ubyte.gz: training set labels (28881 bytes)<br><br>- t10k-images-idx3-ubyte.gz: test set images (1648877 bytes)<br><br>-  t10k-labels-idx1-ubyte.gz: test set labels (4542 bytes)<br>  &gt; 网址: http://yann.lecun.com/exdb/mnist<br><br>### 1 特征值<br><br>Mnist数据集可以从官网下载，网址: http://yann.lecun.com/exdb/mnist/下载下来的数据集被分成两部分:55000行的训练数据集(mnist.train)和10000行的测试数据集 (mnist.test)。每一个MNIST数据单元有两部分组成:一张包含手写数字的图片和一个对应的标签。我们把这些图片设为&quot;xs&quot;，把这些标签设为&quot;ys”。训练数据集和测试数据集都包含xs和ys，比如训练数据集的图片是mnist.train.images，训练数据集的标签是mnist.train.labels。<br>![snipaste20230809_201248](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_201248.jpg)<br><br>我们可以知道图片是黑白图片，每一张图片包含28像素X28像素。我们把这个数组展开成一个向量，长度是28x28 = 784。因此，在MNIST训练数据集中，mnist.trajn.images是一个形状为[60000,784]的张量。<br><br>![snipaste20230809_201312](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_201312.jpg)<br><br>### 2 目标值<br><br>MNIST中的每个图像都具有相应的标签，**目标值：0到9之间的数字表示图像中绘制的数字**。用的是one-hot编码<br>nn[0,0,0,1,0,0,0,0,0,0] mnist.train.labels [55000，10]<br><br>![snipaste20230809_201431](https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_201431.jpg)<br><br>### 3.7.2 minst数据获取API<br><br>TensorFlow框架自带了获取这个数据集的接口，所以不需要自行读取。<br><br>- from tensorflow.examples.tutorials.mnist import input_data<br>  - mnist = input_data.read_data_sets(path, one_hot=True)<br>    - mnist.train.next_batch(100)(提供批量获取功能)<br>    - mnist.train.images、labels<br>    - mnist.test.images、labels<br><br>### 3.7.3 实战：Mnist手写数字识别<br><br>### 1 网络设计<br><br>我们采用只有一层，即最后一个输出层的神经网络，也称之为全连接(full connected) 层神经网络<br><br>&lt;img src=&quot;https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_201819.jpg&quot; alt=&quot;snipaste20230809_201819&quot; style=&quot;zoom:33%;&quot; /&gt;<br><br>### 2 全连接层计算：即构造权重和偏置<br><br>- tf.matmul(a, b, name=None)+bias<br><br>  - return:全连接结果，供交叉损失运算<br>- tf.train.GradientDescentOptimizer(learning_rate) ：梯度下降（优化交叉熵）<br>    - learning_rate:学习率<br>    - method: minimize(loss):最小优化损失<br><br>```  1 特征值<br>            全连接<br>            [None, 784] * W[784, 10] + Bias = [None, 10]<br>            构建全连接层：<br>            y_predict = tf.matmul(x, W) + Bias<br>            构造损失：<br>            loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict,name=None)<br>            如何计算准确率?<br>            np.argmax(y_predict, axis=1)<br>            tf.argmax(y_true, axis=1)<br>                y_predict [None, 10]<br>                y_true [None, 10]<br>            tf.equal()<br></code></pre></td></tr></table></figure>
<h3 id="3-代码">3 代码</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">def full_connection():<br>	# 用全连接对手写数字进行识别<br>	# 1、准备数据<br>	mnist=input_data.read_data_sets(&quot;./data/mnist_data&quot;,one_hot = True)<br>	#  用占位符定义真实数据<br>	X=tf.placeholder(dtype = tf.float32,shape = [None,784]) # 特征值<br>	y_true=tf.placeholder(dtype = tf.float32,shape = [None,10]) # 真实值<br><br>	# 2、构建模型 - 全连接<br>	# y_predict[None,10]=X[None,784]*weights[784,10]+bias[10]<br>	weights=tf.Variable(initial_value = tf.random_normal(shape = [784,10],stddev = 0.01))<br>	bias=tf.Variable(initial_value = tf.random_normal(shape = [10],stddev = 0.1))<br>	y_predict=tf.matmul(X,weights)+bias<br><br>	# 3、构造损失函数<br>	loss_list=tf.nn.softmax_cross_entropy_with_logits(logits = y_predict,labels = y_true)<br>	loss=tf.reduce_mean(loss_list)<br><br>	# 4、优化损失(梯度下降)<br>	optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)<br><br>	# 5、计算准确率 tf.argmax()计算最大值所在列数<br>	bool_list=tf.equal(tf.argmax(y_predict,axis = 1),tf.argmax(y_true,axis = 1))<br>	accuracy=tf.reduce_mean(tf.cast(bool_list,tf.float32))<br>	# 初始化变量<br>	init = tf.global_variables_initializer()<br><br>	# 开启会话<br>	with tf.Session() as sess:<br>		# 初始化变量<br>		sess.run(init)<br><br>		# 开始训练<br>		for i in range(100):<br>			# 获取真实值<br>			image, label = mnist.train.next_batch(100)<br>			# 因为optimizer返回的是None 所以用_,来接<br>			_, loss_value, accuracy_value = sess.run([optimizer, loss, accuracy], feed_dict = &#123;X: image, y_true: label&#125;)<br><br>			print(&quot;第%d次的损失为%f，准确率为%f&quot; % (i + 1, loss_value, accuracy_value))<br><br>	return None<br><br>if __name__ == &#x27;__main__&#x27;:<br>    # 如果你报numpy.ufunc size changed错误，尝试更新一下Numpy的版本<br>    # pip install numpy==1.16.0<br>	full_connection()<br>    <br>&#x27;&#x27;&#x27;<br>第1次的损失为2.292725，准确率为0.160000<br>第2次的损失为2.331739，准确率为0.020000<br>…………………………<br>第100次的损失为1.510217，准确率为0.730000<br>&#x27;&#x27;&#x27;    <br><br></code></pre></td></tr></table></figure>
<h4 id="4-准确率计算">4 准确率计算</h4>
<p>如何提高准确率？<br>
1）增加训练次数<br>
2）调节学习率<br>
3）调节权重系数的初始化值<br>
4）改变优化器</p>
<h2 id="4-1-卷积神经网络简介">4.1 卷积神经网络简介</h2>
<h3 id="4-1-1-卷积神经网络与传统多层神经网络对比">4.1.1 卷积神经网络与传统多层神经网络对比</h3>
<ul>
<li>
<p>传统意义上的多层神经网络只有输入层 隐藏层 输出层 其中隐藏层的层数根据需要而定，没有明确的理论推导来说明到底多少层合适</p>
</li>
<li>
<p><strong>卷积神经网络</strong>（Convolutional Neural Networks, CNN），在原来多层神经网络的基础上，加入了更加有效的特征学习部分，具体操作就是<strong>在原来的全连接层前面加入了卷积层、激活层和池化层</strong>。卷积神经网络出现，使得神经网络层数得以加深，“深度”学习由此而来。</p>
</li>
</ul>
<blockquote>
<p>通常所说的深度学习，一般指的是这些CNN等新的结构以及一些新的方法（比如新的激活函数Relu等)，解决了传统多层神经网络的一些难以解决的问题</p>
</blockquote>
<h3 id="4-1-2-发展历史">4.1.2 发展历史</h3>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_203917.jpg" alt="snipaste20230809_203917"></p>
<ol>
<li>网络结构加深</li>
<li>增强卷积模块功能</li>
<li>从分类任务到检测任务</li>
<li>增加新的功能单元</li>
</ol>
<h3 id="4-1-3-卷积神经网络ImageNet比赛错误率">4.1.3 卷积神经网络ImageNet比赛错误率</h3>
<blockquote>
<p>lmageNet可以说是计算机视觉研究人员进行大规模物体识别和检测时，最先想到的视觉大数据来源，最初由斯坦福大学李飞飞等人在CVPR 2009的一篇论文中推出，并坡用于替代 PASCAL数据集（后者在数据规模和多样性上都不如lmageNet)和LabelMe数据集(在标准化上不如 ImageNet) 。</p>
<p><strong>lmageNet不但是计算机视觉发展的重要推动者，也是这一波深度学习热潮的关键驱动力之一。</strong></p>
<p>截至2016年,ImageNet中含有超过1500万由人手工注释的图片网址，也就是带标签的图片，标签说明了图片中的内容，超过2.2万个类别。</p>
</blockquote>
<h2 id="4-2-卷积神经网络原理">4.2 卷积神经网络原理</h2>
<h3 id="4-2-1-卷积神经网络三个结构">4.2.1 卷积神经网络三个结构</h3>
<p>神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层、激活层和池化层(pooling layer，又叫下采样层subsample)。</p>
<ul>
<li>
<p>卷积层: 通过在原始图像上平移来提取特征</p>
</li>
<li>
<p>激活层: 增加非线性分割能力</p>
</li>
<li>
<p>池化层(下采样层): 减少学习的参数，降低网络的复杂度(最大池化和平均池化)</p>
</li>
<li>
<p>**全连接层：**实现分类效果</p>
</li>
<li>
<p>**输出层：**进行损失计算并输出分类结果</p>
</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_204500.jpg" alt="snipaste20230809_204500"></p>
<h3 id="4-2-2-卷积层">4.2.2 卷积层</h3>
<p><strong>卷积神经网络中每层卷积层由若干卷积单元(卷积核)组成</strong>，每个卷积单元的参数都是通过反向传播算法最佳化得到的。</p>
<p><strong>卷积运算的目的是特征提取</strong>，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</p>
<p>卷积核又叫 filter、过滤器、模型参数、卷积单元</p>
<h3 id="1-卷积核-Filter-的四大要素">1 <strong>卷积核(Filter)的四大要素</strong></h3>
<ol>
<li>卷积核个数</li>
<li>卷积核大小</li>
<li>卷积核步长</li>
<li>卷积核零填充大小</li>
</ol>
<h3 id="2-卷积核计算-大小">2 <strong>卷积核计算-大小</strong></h3>
<p>卷积核大小一般为 1 * 1 3 * 3 5 * 5</p>
<p>卷积核我们可以理解为一个观察的人，带着若干权重和一个偏置去观察，进行特征加权运算。</p>
<p>下图只有权重，少了偏置，一般情况下都要加上偏置！</p>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_204803.jpg" alt="snipaste20230809_204803" style="zoom:33%;" />
<p>通常的卷积核大小 1,3,5，是经过研究人员实验证明比较好的效果。这个人观察之后会得到一个运算结果，<br>
那么这个人想观察所有这张图的像素怎么办?那就需要平移（即步长）:</p>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_204815.jpg" alt="snipaste20230809_204815" style="zoom:33%;" />
<h3 id="3-步长">3 <strong>步长</strong></h3>
<p>需要去平移卷积核观察这张图片，需要的参数就是<strong>步长</strong>。</p>
<p>假设移动的步长为一个像素，那么最终这个人观察的结果以下图为例:</p>
<p>5x5的图片，3x3的卷积大小去<strong>一个步长</strong>运算得到3x3的大小观察结果</p>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_205016.jpg" alt="snipaste20230809_205016" style="zoom:50%;" />
<p>如果移动的步长为2那么结果是这样</p>
<p>5x5的图片，3x3的卷积大小，2个步长运算得到2x2的大小观察结果</p>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_205027.jpg" alt="snipaste20230809_205027" style="zoom: 50%;" />
<h3 id="4-卷积核个数">4 卷积核个数</h3>
<blockquote>
<p>不同的卷积核带的权重和偏置都不一样，即随机初始化的参数</p>
</blockquote>
<p>那么如果在某一层结构当中，不止是一个人观察，多个人(卷积核)一起去观察。那就得到多张观察结果。</p>
<p>也就是说，一个卷积核得到一个观察结果，多个卷积核就是多个观察结果</p>
<h3 id="5-零填充大小">5 零填充大小</h3>
<p>Filter观察窗口的大小和移动步长有时会导致超过图片像素宽度!</p>
<p>解决办法：</p>
<ol>
<li>不要超出图像宽度的观察结果</li>
<li>零填充</li>
</ol>
<blockquote>
<p>零填充就是在图片像素外围填充一圈值为0的像素。</p>
</blockquote>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_205439.jpg" alt="snipaste20230809_205439" style="zoom:50%;" />
<h3 id="6-计算输出大小">6 计算输出大小</h3>
<p>如果已知输入图片形状，卷积核数量，卷积核大小，以及移动步长，那么输出图片形状如何确定?</p>
<p>H1:图像大小 就那三个参数 D1通道数</p>
<ul>
<li>输入体积大小 <em><em>H1</em> W1 * D1</em>*</li>
<li>四个超参数︰
<ul>
<li>Filter数量K</li>
<li>Filter大小F</li>
<li>步长S</li>
<li>零填充大小P</li>
</ul>
</li>
<li>输出体积大小 <em><em>H2 * W2</em> D2</em>*
<ul>
<li>H2=(H1- F＋2P)/S+1 ((高-过滤器大小+2零填充)/步长+1)</li>
<li>W2= (W1- F+2P)/S+1</li>
<li>D2=k</li>
</ul>
</li>
</ul>
<p>计算案例:</p>
<p>1、假设已知的条件:输入图像32 * 32 * 1 , 50个Filter，大小为5 * 5，移动步长为1，零填充大小为1。请求出输出大小?<br>
H2= (H1 -F +2P)/S + 1 =(32 - 5+2 * 1)/1 + 1 = 30</p>
<p>w2=(W1-F+2P)/S + 1 =(32 -5+2 * 1)/1 + 1 = 30</p>
<p>D2= K= 50（D2等于过滤器的数量K）<br>
所以输出大小为[30，30，50]</p>
<p>2、假设已知的条件∶输入图像32 * 32 * 1 ,50个Filter，大小为3*3，移动步长为1，未知零填充。输出大小32 * 32，求零填充大小?<br>
H2= (H1 -F +2P)/S + 1 = (32 - 3+2 * P)/1 +1 = 32</p>
<p>w2=(W1 -F+2P)/S+ 1 =(32-3+2 * P)/1 +1 = 32</p>
<p>故P=1</p>
<p>所以零填充大小为:1*1</p>
<h3 id="7-多通道图片如何观察">7 多通道图片如何观察</h3>
<p>如果是一张彩色图片，那么就有三种表分别为R，G，B。原本每个人需要带一个3x3或者其他大小的卷积核，现在需要带3张3x3的权重和一个偏置，总共就27个权重。最终每个人还是得出一张结果:</p>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230809_205626.jpg" alt="snipaste20230809_205626" style="zoom:50%;" />
<h3 id="8-卷积网络API">8 <strong>卷积网络API</strong></h3>
<ul>
<li>
<p>tf.nn.conv2d(input, filter, strides=, padding=, name=None)</p>
</li>
<li>
<p>计算给定4-D input和filter张量的2维卷积</p>
</li>
<li>
<p>input:给定的输入张量，具有[batch,heigth,width,channel]，类型一定为 float32,64</p>
</li>
<li>
<p>filter:指定过滤器的权重数量，[filter_height, filter_width, in_channels,out_channels]</p>
</li>
<li>
<p>strides: strides = [1, stride,stride,1],步长。 例如，如果步长为1，则strides=[1,1,1,1]</p>
</li>
<li>
<p>padding: “SAME&quot;“，“VALID”，具体解释见下面。</p>
</li>
<li>
<p>Tensorflow的零填充方式有两种方式，SAME和VALID</p>
<ul>
<li>
<p>SAME︰越过边缘取样，取样的面积和输入图像的像素宽度一致。公式:ceil(H/S)</p>
<ul>
<li>
<p>H为输入的图片的高或者宽，S为步长</p>
</li>
<li>
<p>无论过滤器的大小是多少，零填充的数量由API自动计算。</p>
</li>
</ul>
</li>
<li>
<p>VALID:不越过边缘取样，取样的面积小于输入人的图像的像素宽度。不填充。、</p>
<ul>
<li>一般填same , valid如果越过边缘的话，观察结果直接丢弃掉<br>
在Tensorflow当中，卷积API设置&quot;SAME”之后，如果步长为1，输出高宽与输入大小一样(重要)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">卷积网络API<br>    tf.nn.conv2d(input, filter, strides=, padding=)<br>    input：输入图像<br>        要求：形状[batch,heigth,width,channel]<br>        类型为float32,64<br>    filter:<br>        weights<br>        变量initial_value=random_normal(shape=[F, F, 3/1, K])<br>    strides:<br>        步长 1<br>         [1, 1, 1, 1]<br>    padding: “SAME”<br>        “SAME”：越过边缘取样<br>        “VALID”：不越过边缘取样<br>1）掌握filter要素的相关计算公式<br>2）filter大小<br>    1x1，3x3，5x5<br>  步长 1<br>3）每个过滤器会带有若干权重和1个偏置<br></code></pre></td></tr></table></figure>
<h3 id="4-2-3-激活函数">4.2.3 激活函数</h3>
<p>随着神经网络的发展，大家发现原有的sigmoid等激活函数并不能达到好的效果，所以才去新的激活函数。</p>
<ul>
<li>Relu</li>
<li>Tanh</li>
<li>sigmoid</li>
</ul>
<p>不同激活函数网站演示：<a target="_blank" rel="noopener" href="http://playground.tensorflow.org">http://playground.tensorflow.org</a></p>
<h4 id="1-Relu">1 Relu</h4>
<img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230810_194226.jpg" alt="snipaste20230810_194226" style="zoom:50%;" />
<p>效果：</p>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230810_194235.jpg" alt="snipaste20230810_194235"></p>
<h4 id="2-Relu激活函数API">2 Relu激活函数API</h4>
<ul>
<li>tf.nn.relu(features, name=None)
<ul>
<li>features: 卷积后加上偏置的结果</li>
<li>return: 激活函数的计算结果</li>
</ul>
</li>
</ul>
<h4 id="为什么采用新的激活函数"><strong>为什么采用新的激活函数</strong></h4>
<ul>
<li>Relu优点
<ul>
<li>有效解决梯度消失问题</li>
<li>计算速度非常快，只 需要判断输入是否大于0。SGD(批梯度下降)的求解速度速度远快于sigmoid和tanh</li>
</ul>
</li>
<li>sigmoid缺点
<ul>
<li>采用sigmoid等函数，计算量相对大，而采用Relu 激活函数，整个过程的计算量节省很多。在深层网络中，sigmoid函数反向传播时，很容易就会出现梯度消失的情况</li>
</ul>
</li>
</ul>
<h3 id="4-2-4-池化层-Polling">4.2.4 池化层(Polling)</h3>
<p>Pooling层主要的作用是特征提取，通过去掉Feature Map中不重要的样本，进一步减少参数数量。即通过池化层尽可能保留图像的主要特征，不会影响图像模型的最终效果，减少模型的复杂度，避免过拟合现象。</p>
<p>Pooling的方法很多，通常采用最大池化</p>
<ul>
<li>max_polling:取池化窗口的最大值</li>
<li>avg_polling:取池化窗口的平均值</li>
</ul>
<p><img src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230810_195138.jpg" alt="snipaste20230810_195138"></p>
<h3 id="1-池化层计算">1 池化层计算</h3>
<p>池化层计算： H2=(H1- F＋2P)/S+1 ((高-过滤器大小+2零填充)/步长+1)</p>
<p>池化层也有窗口的大小以及移动步长，那么之后的输出大小怎么计算? 计算公式同卷积计算公式一样</p>
<blockquote>
<p>计算:224x224x64,窗口为2，步长为2 输出结果?</p>
<p>H2 =(224 -2+ 2 * 0)/2 +1 = 112<br>
w2 =(224- 2+ 2 * 0)/2 +1 =112</p>
</blockquote>
<p><strong>通常池化层采用2x2大小、步长为2窗口，零填充为默认P=0</strong></p>
<p>输入图片大小为200×200，依次经过一层卷积(kernel size 5×5，padding 1，stride 2) ,<br>
pooling (kernel size 3×3，padding 0，stride 1)，又一层卷积(kernel size 3×3，padding 1，stride 1)之后，</p>
<p>输出特征图大小为:<br>
A.95 B.96 C.97 D.98 E.99 F.100</p>
<p>答案:C</p>
<p>h2=99 w2=99 d2=5 h3=97 d3=3 h4=97 d4=3</p>
<p><strong>注:卷积层向下取整，池化层向上取整</strong></p>
<h3 id="2-池化层API">2 池化层API</h3>
<ul>
<li>tf.nn.max_pool(value,ksize=, strides=, padding=,name=None)
<ul>
<li>输入上执行最大池数</li>
<li>value: 4-D Tensor形状[batch, height,width, channels]</li>
<li>channel:并不是原始图片的通道数，而是多少filter观察</li>
<li>ksize:池化窗口大小，[1，ksize, ksize,1]</li>
<li>strides:步长大小，[1,strides,strides,1]</li>
<li>padding:“SAME”&quot;，“VALID”，使用的填充算法的类型，默认使用 “SAME”</li>
</ul>
</li>
</ul>
<h3 id="4-2-5-全连接层">4.2.5 全连接层</h3>
<p><strong>全连接层计算：即构造权重和偏置</strong></p>
<ul>
<li>tf.matmul(a, b, name=None)+bias
<ul>
<li>return:全连接结果，供交叉损失运算</li>
</ul>
</li>
<li>tf.train.GradientDescentOptimizer(learning_rate) ：梯度下降（优化交叉熵）
<ul>
<li>learning_rate:学习率</li>
<li>method: minimize(loss):最小优化损失</li>
</ul>
</li>
</ul>
<p>y=w1x1+w2x2+…+b</p>
<p>y_predict[None,10]=X[None,784]*weights[784,10]+bias[10]</p>
<h2 id="4-3-验证码识别">4.3 验证码识别</h2>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://gxblogs.com">ggw &amp; xpl</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://gxblogs.com/posts/48513/">https://gxblogs.com/posts/48513/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://gxblogs.com" target="_blank">GXBLOGS</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/ggw2021/images@main/blogs/Snipaste_2024-01-24_00-26-26.png" data-sites="wechat,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-03-06.png" target="_blank"><img class="post-qr-code-img" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-03-06.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-04-50.png" target="_blank"><img class="post-qr-code-img" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2023-01-05_13-04-50.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/16719/"><img class="prev-cover" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/blogs/Snipaste_2024-03-01_14-23-35.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">C语言笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/17963/"><img class="next-cover" src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/ggw2021/images@main/blogs/Snipaste_2024-01-23_18-54-20.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LATEX半小时速通（适合模版改写）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/11163/" title="人工智能（东软）"><img class="cover" src="/static/imgs/loading.gif" data-original="https://ggwimgs-1313043536.cos.ap-guangzhou.myqcloud.com/python/ai2023learning/Snipaste_2023-02-20_11-44-21.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-20</div><div class="title">人工智能（东软）</div></div></a></div><div><a href="/posts/498ab7d9/" title="机器学习"><img class="cover" src="/static/imgs/loading.gif" data-original="https://cdn.jsdelivr.net/gh/paipai2001/img@main/blogs/snipaste20230717_112943.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-20</div><div class="title">机器学习</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">三天入门深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">1.1 深度学习与机器学习的区别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-1-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%96%B9%E9%9D%A2"><span class="toc-text">1.1.1 特征提取方面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-2-%E6%95%B0%E6%8D%AE%E9%87%8F%E5%92%8C%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD%E8%A6%81%E6%B1%82"><span class="toc-text">1.1.2 数据量和计算性能要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-3-%E7%AE%97%E6%B3%95%E4%BB%A3%E8%A1%A8"><span class="toc-text">1.1.3 算法代表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">1.2 深度学习的应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D"><span class="toc-text">1.3 深度学习框架介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-%E5%B8%B8%E8%A7%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94"><span class="toc-text">1.3.1 常见深度学习框架对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-2-TensorFlow-%E7%9A%84%E7%89%B9%E7%82%B9"><span class="toc-text">1.3.2 TensorFlow 的特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-3-Tensorflow%E7%9A%84%E5%AE%89%E8%A3%85"><span class="toc-text">1.3.3 Tensorflow的安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-CPU%E7%89%88%E6%9C%AC"><span class="toc-text">1 CPU版本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-GPU%E7%89%88%E6%9C%AC"><span class="toc-text">2 GPU版本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-TF%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE"><span class="toc-text">2.2 TF数据流图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E6%A1%88%E4%BE%8B%EF%BC%9ATensorflow%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8A%A0%E6%B3%95%E8%BF%90%E7%AE%97"><span class="toc-text">2.2.1 案例：Tensorflow实现一个加法运算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%A3%E7%A0%81"><span class="toc-text">1 代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-text">结果：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Tensorflow-%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-text">2 Tensorflow 结构分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE%E4%BB%8B%E7%BB%8D"><span class="toc-text">2.1.2 数据流图介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%9B%BE%E7%BB%93%E6%9E%84"><span class="toc-text">2.2.1 什么是图结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-%E5%9B%BE%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C"><span class="toc-text">2.2.2 图相关操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%BB%98%E8%AE%A4%E5%9B%BE"><span class="toc-text">1 默认图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%88%9B%E5%BB%BA%E5%9B%BE"><span class="toc-text">2 创建图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3-TensorBoard-%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text">2.2.3 TensorBoard:可视化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96-events%E6%96%87%E4%BB%B6"><span class="toc-text">1 数据序列化-events文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%90%AF%E5%8A%A8TensorBoard"><span class="toc-text">2 启动TensorBoard</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4-OP"><span class="toc-text">2.2.4 OP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%B8%B8%E8%A7%81OP"><span class="toc-text">1 常见OP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%8C%87%E4%BB%A4%E5%90%8D%E7%A7%B0"><span class="toc-text">2 指令名称</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B"><span class="toc-text">3.1 文件读取流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B"><span class="toc-text">3.1.1 文件读取流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%9E%84%E9%80%A0%E6%96%87%E4%BB%B6%E5%90%8D%E9%98%9F%E5%88%97"><span class="toc-text">1 构造文件名队列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AF%BB%E5%8F%96%E4%B8%8E%E8%A7%A3%E7%A0%81"><span class="toc-text">2 读取与解码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89-%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9"><span class="toc-text">1） 读取文件内容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89-%E5%86%85%E5%AE%B9%E8%A7%A3%E7%A0%81"><span class="toc-text">2） 内容解码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-text">3 批处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E7%BA%BF%E7%A8%8B%E6%93%8D%E4%BD%9C"><span class="toc-text">3.1.2 线程操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE"><span class="toc-text">3.2 图片数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E5%9B%BE%E5%83%8F%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="toc-text">3.2.1 图像基本知识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%9B%BE%E7%89%87%E4%B8%89%E8%A6%81%E7%B4%A0"><span class="toc-text">1 图片三要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6"><span class="toc-text">2 张量形状</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E5%9B%BE%E7%89%87%E7%89%B9%E5%BE%81%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-text">3.2.2 图片特征值处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-text">3.2.3 数据格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%8B%97%E5%9B%BE%E7%89%87%E8%AF%BB%E5%8F%96%E6%A1%88%E4%BE%8B"><span class="toc-text">3.2.4 案例：狗图片读取案例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-text">1 读取流程分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%A3%E7%A0%81"><span class="toc-text">2 代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BB%A3%E7%A0%81"><span class="toc-text">3 代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%87%86%E7%A1%AE%E7%8E%87%E8%AE%A1%E7%AE%97"><span class="toc-text">4 准确率计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E4%BB%8B"><span class="toc-text">4.1 卷积神经网络简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E4%BC%A0%E7%BB%9F%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E6%AF%94"><span class="toc-text">4.1.1 卷积神经网络与传统多层神经网络对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="toc-text">4.1.2 发展历史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CImageNet%E6%AF%94%E8%B5%9B%E9%94%99%E8%AF%AF%E7%8E%87"><span class="toc-text">4.1.3 卷积神经网络ImageNet比赛错误率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86"><span class="toc-text">4.2 卷积神经网络原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%89%E4%B8%AA%E7%BB%93%E6%9E%84"><span class="toc-text">4.2.1 卷积神经网络三个结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-text">4.2.2 卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%8D%B7%E7%A7%AF%E6%A0%B8-Filter-%E7%9A%84%E5%9B%9B%E5%A4%A7%E8%A6%81%E7%B4%A0"><span class="toc-text">1 卷积核(Filter)的四大要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8D%B7%E7%A7%AF%E6%A0%B8%E8%AE%A1%E7%AE%97-%E5%A4%A7%E5%B0%8F"><span class="toc-text">2 卷积核计算-大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%AD%A5%E9%95%BF"><span class="toc-text">3 步长</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%8D%B7%E7%A7%AF%E6%A0%B8%E4%B8%AA%E6%95%B0"><span class="toc-text">4 卷积核个数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E9%9B%B6%E5%A1%AB%E5%85%85%E5%A4%A7%E5%B0%8F"><span class="toc-text">5 零填充大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E8%AE%A1%E7%AE%97%E8%BE%93%E5%87%BA%E5%A4%A7%E5%B0%8F"><span class="toc-text">6 计算输出大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%A4%9A%E9%80%9A%E9%81%93%E5%9B%BE%E7%89%87%E5%A6%82%E4%BD%95%E8%A7%82%E5%AF%9F"><span class="toc-text">7 多通道图片如何观察</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9CAPI"><span class="toc-text">8 卷积网络API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">4.2.3 激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Relu"><span class="toc-text">1 Relu</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Relu%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0API"><span class="toc-text">2 Relu激活函数API</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%87%87%E7%94%A8%E6%96%B0%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">为什么采用新的激活函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-%E6%B1%A0%E5%8C%96%E5%B1%82-Polling"><span class="toc-text">4.2.4 池化层(Polling)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%B1%A0%E5%8C%96%E5%B1%82%E8%AE%A1%E7%AE%97"><span class="toc-text">1 池化层计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%B1%A0%E5%8C%96%E5%B1%82API"><span class="toc-text">2 池化层API</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="toc-text">4.2.5 全连接层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB"><span class="toc-text">4.3 验证码识别</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: rgb(112, 112, 112)"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By ggw & xpl</div><div class="footer_custom_text">都是科技与狠活啊</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'QvVqZWBQWj9Aq2xrw4Z8z8Pz-gzGzoHsz',
      appKey: 'COANudzj20V6IrCfAsD1Ufya',
      avatar: 'robohash',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/js/ggwsettings.js"></script><script>(function(d, w, c) {
    w.ChatraID = 'dKkRxvMac7f9AeKhu';
    var s = d.createElement('script');
    w[c] = w[c] || function() {
        (w[c].q = w[c].q || []).push(arguments);
    };
    s.async = true;
    s.src = 'https://call.chatra.io/chatra.js';
    if (d.head) d.head.appendChild(s);
})(document, window, 'Chatra');

if (true) {
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      Chatra('openChat')
    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      Chatra('hide')
    }
    function chatBtnShow () {
      Chatra('show')
    }
  }
}</script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>